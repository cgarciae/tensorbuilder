<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>tensorbuilder.builder_nn API documentation</title>
    <meta name="description" content="All functions in this module are automatically generated. They help create custom layers and mapping..." />

  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 1em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  } 

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; } 

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;
      
      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
</head>
<body>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <h1>Index</h1>
    <ul id="index">

    <li class="set"><h3><a href="#header-functions">Functions</a></h3>
      
  <ul>
    <li class="mono"><a href="#tensorbuilder.builder_nn.all_candidate_sampler_layer">all_candidate_sampler_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.avg_pool_layer">avg_pool_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.batch_norm_with_global_normalization_layer">batch_norm_with_global_normalization_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.batch_normalization_layer">batch_normalization_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.bias_add_grad_layer">bias_add_grad_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.bias_add_layer">bias_add_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.bias_add_v1_layer">bias_add_v1_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.bidirectional_rnn_layer">bidirectional_rnn_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.compute_accidental_hits_layer">compute_accidental_hits_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.conv2d_backprop_filter_layer">conv2d_backprop_filter_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.conv2d_backprop_input_layer">conv2d_backprop_input_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.conv2d_layer">conv2d_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.conv2d_transpose_layer">conv2d_transpose_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.depthwise_conv2d_layer">depthwise_conv2d_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.depthwise_conv2d_native_backprop_filter_layer">depthwise_conv2d_native_backprop_filter_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.depthwise_conv2d_native_backprop_input_layer">depthwise_conv2d_native_backprop_input_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.depthwise_conv2d_native_layer">depthwise_conv2d_native_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.dropout_layer">dropout_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.dynamic_rnn_layer">dynamic_rnn_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.elu_layer">elu_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.embedding_lookup_layer">embedding_lookup_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.embedding_lookup_sparse_layer">embedding_lookup_sparse_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.fixed_unigram_candidate_sampler_layer">fixed_unigram_candidate_sampler_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.in_top_k_layer">in_top_k_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.l2_loss_layer">l2_loss_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.l2_normalize_layer">l2_normalize_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.learned_unigram_candidate_sampler_layer">learned_unigram_candidate_sampler_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.local_response_normalization_layer">local_response_normalization_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.log_softmax_layer">log_softmax_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.log_uniform_candidate_sampler_layer">log_uniform_candidate_sampler_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.lrn_layer">lrn_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.make_all_layer">make_all_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map__compute_sampled_logits">map__compute_sampled_logits</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map__sum_rows">map__sum_rows</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_all_candidate_sampler">map_all_candidate_sampler</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_avg_pool">map_avg_pool</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_batch_norm_with_global_normalization">map_batch_norm_with_global_normalization</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_batch_normalization">map_batch_normalization</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_bias_add">map_bias_add</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_bias_add_grad">map_bias_add_grad</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_bias_add_v1">map_bias_add_v1</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_bidirectional_rnn">map_bidirectional_rnn</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_compute_accidental_hits">map_compute_accidental_hits</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_conv2d">map_conv2d</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_conv2d_backprop_filter">map_conv2d_backprop_filter</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_conv2d_backprop_input">map_conv2d_backprop_input</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_conv2d_transpose">map_conv2d_transpose</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_depthwise_conv2d">map_depthwise_conv2d</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_depthwise_conv2d_native">map_depthwise_conv2d_native</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_depthwise_conv2d_native_backprop_filter">map_depthwise_conv2d_native_backprop_filter</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_depthwise_conv2d_native_backprop_input">map_depthwise_conv2d_native_backprop_input</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_dropout">map_dropout</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_dynamic_rnn">map_dynamic_rnn</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_elu">map_elu</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_embedding_lookup">map_embedding_lookup</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_embedding_lookup_sparse">map_embedding_lookup_sparse</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_fixed_unigram_candidate_sampler">map_fixed_unigram_candidate_sampler</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_in_top_k">map_in_top_k</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_l2_loss">map_l2_loss</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_l2_normalize">map_l2_normalize</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_learned_unigram_candidate_sampler">map_learned_unigram_candidate_sampler</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_local_response_normalization">map_local_response_normalization</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_log_softmax">map_log_softmax</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_log_uniform_candidate_sampler">map_log_uniform_candidate_sampler</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_lrn">map_lrn</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_make_all">map_make_all</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_max_pool">map_max_pool</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_max_pool_with_argmax">map_max_pool_with_argmax</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_moments">map_moments</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_nce_loss">map_nce_loss</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_normalize_moments">map_normalize_moments</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_relu">map_relu</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_relu6">map_relu6</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_relu_layer">map_relu_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_rnn">map_rnn</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_sampled_softmax_loss">map_sampled_softmax_loss</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_separable_conv2d">map_separable_conv2d</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_sigmoid">map_sigmoid</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_sigmoid_cross_entropy_with_logits">map_sigmoid_cross_entropy_with_logits</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_softmax">map_softmax</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_softmax_cross_entropy_with_logits">map_softmax_cross_entropy_with_logits</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_softplus">map_softplus</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_softsign">map_softsign</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_sparse_softmax_cross_entropy_with_logits">map_sparse_softmax_cross_entropy_with_logits</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_state_saving_rnn">map_state_saving_rnn</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_sufficient_statistics">map_sufficient_statistics</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_tanh">map_tanh</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_top_k">map_top_k</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_uniform_candidate_sampler">map_uniform_candidate_sampler</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_weighted_cross_entropy_with_logits">map_weighted_cross_entropy_with_logits</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_xw_plus_b">map_xw_plus_b</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_xw_plus_b_v1">map_xw_plus_b_v1</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.map_zero_fraction">map_zero_fraction</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.max_pool_layer">max_pool_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.max_pool_with_argmax_layer">max_pool_with_argmax_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.moments_layer">moments_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.nce_loss_layer">nce_loss_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.normalize_moments_layer">normalize_moments_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.relu6_layer">relu6_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.relu_layer">relu_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.relu_layer_layer">relu_layer_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.rnn_layer">rnn_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.sampled_softmax_loss_layer">sampled_softmax_loss_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.separable_conv2d_layer">separable_conv2d_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.sigmoid_cross_entropy_with_logits_layer">sigmoid_cross_entropy_with_logits_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.sigmoid_layer">sigmoid_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.softmax_cross_entropy_with_logits_layer">softmax_cross_entropy_with_logits_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.softmax_layer">softmax_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.softplus_layer">softplus_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.softsign_layer">softsign_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.sparse_softmax_cross_entropy_with_logits_layer">sparse_softmax_cross_entropy_with_logits_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.state_saving_rnn_layer">state_saving_rnn_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.sufficient_statistics_layer">sufficient_statistics_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.tanh_layer">tanh_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.top_k_layer">top_k_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.uniform_candidate_sampler_layer">uniform_candidate_sampler_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.use_extras">use_extras</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.weighted_cross_entropy_with_logits_layer">weighted_cross_entropy_with_logits_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.xw_plus_b_layer">xw_plus_b_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.xw_plus_b_v1_layer">xw_plus_b_v1_layer</a></li>
    <li class="mono"><a href="#tensorbuilder.builder_nn.zero_fraction_layer">zero_fraction_layer</a></li>
  </ul>

    </li>

    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#tensorbuilder.builder_nn.DefaultArgSpec">DefaultArgSpec</a></span>
        
        </li>
      </ul>
    </li>

    </ul>
  </div>

    <article id="content">
      
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">tensorbuilder.builder_nn</span> module</h1>
  <p>All functions in this module are automatically generated. They help create custom layers and mappings for a Builder based on the functions in <code>tf.nn</code>. It works the following way:</p>
<ul>
<li>Let <code>f</code> be a function in <code>tf.nn</code>, then the functions with name <code>f_layer</code> and <code>map_f</code> exists in this module and take a Builder as its first argument</li>
<li><code>f_layer</code> functions connect a Builder to a layer with <code>f</code> as its activation function.</li>
<li><code>map_f</code> functions just map <code>f</code> over the tensor inside the Builder.</li>
</ul>
<p>Calling <a href="#tensorbuilder.builder_nn.use_extras"><code>use_extras</code></a> monkey-patches all the functions in this module as methods of the Builder class.</p>
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builder_nn', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builder_nn" class="source">
    <div class="codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">All functions in this module are automatically generated. They help create custom layers and mappings for a Builder based on the functions in `tf.nn`. It works the following way:</span>

<span class="sd">* Let `f` be a function in `tf.nn`, then the functions with name `f_layer` and `map_f` exists in this module and take a Builder as its first argument</span>
<span class="sd">* `f_layer` functions connect a Builder to a layer with `f` as its activation function.</span>
<span class="sd">* `map_f` functions just map `f` over the tensor inside the Builder.</span>

<span class="sd">Calling `tensorbuilder.builder_nn.use_extras` monkey-patches all the functions in this module as methods of the Builder class.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorbuilder</span> <span class="kn">as</span> <span class="nn">tb</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>

<span class="n">_patches</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">use_extras</span><span class="p">():</span>
	<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	Moneky-patches all functions in this modules as methods on the Builder class.</span>
<span class="sd">	&quot;&quot;&quot;</span>
	<span class="k">for</span> <span class="n">patch</span> <span class="ow">in</span> <span class="n">_patches</span><span class="p">:</span>
		<span class="k">exec</span><span class="p">(</span><span class="n">patch</span><span class="p">)</span>

<span class="n">DefaultArgSpec</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;DefaultArgSpec&#39;</span><span class="p">,</span> <span class="s1">&#39;has_default default_value&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_get_default_arg</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">defaults</span><span class="p">,</span> <span class="n">arg_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Method that determines if an argument has default value or not,</span>
<span class="sd">    and if yes what is the default value for the argument</span>

<span class="sd">    :param args: array of arguments, eg: [&#39;first_arg&#39;, &#39;second_arg&#39;, &#39;third_arg&#39;]</span>
<span class="sd">    :param defaults: array of default values, eg: (42, &#39;something&#39;)</span>
<span class="sd">    :param arg_index: index of the argument in the argument array for which,</span>
<span class="sd">    this function checks if a default value exists or not. And if default value</span>
<span class="sd">    exists it would return the default value. Example argument: 1</span>
<span class="sd">    :return: Tuple of whether there is a default or not, and if yes the default</span>
<span class="sd">    value, eg: for index 2 i.e. for &quot;second_arg&quot; this function returns (True, 42)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">defaults</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">DefaultArgSpec</span><span class="p">(</span><span class="bp">False</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

    <span class="n">args_with_no_defaults</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">defaults</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">arg_index</span> <span class="o">&lt;</span> <span class="n">args_with_no_defaults</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">DefaultArgSpec</span><span class="p">(</span><span class="bp">False</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">defaults</span><span class="p">[</span><span class="n">arg_index</span> <span class="o">-</span> <span class="n">args_with_no_defaults</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="s1">&#39;&quot;</span><span class="si">%s</span><span class="s1">&quot;&#39;</span> <span class="o">%</span> <span class="n">value</span>
        <span class="k">return</span> <span class="n">DefaultArgSpec</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_get_method_sig</span><span class="p">(</span><span class="n">method</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Given a function, it returns a string that pretty much looks how the</span>
<span class="sd">    function signature would be written in python.</span>

<span class="sd">    :param method: a python method</span>
<span class="sd">    :return: A string similar describing the pythong method signature.</span>
<span class="sd">    eg: &quot;my_method(first_argArg, second_arg=42, third_arg=&#39;something&#39;)&quot;</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># The return value of ArgSpec is a bit weird, as the list of arguments and</span>
    <span class="c1"># list of defaults are returned in separate array.</span>
    <span class="c1"># eg: ArgSpec(args=[&#39;first_arg&#39;, &#39;second_arg&#39;, &#39;third_arg&#39;],</span>
    <span class="c1"># varargs=None, keywords=None, defaults=(42, &#39;something&#39;))</span>
    <span class="n">argspec</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getargspec</span><span class="p">(</span><span class="n">method</span><span class="p">)</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">args</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Use the args and defaults array returned by argspec and find out</span>
    <span class="c1"># which arguments has default</span>
    <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">argspec</span><span class="o">.</span><span class="n">args</span><span class="p">:</span>
        <span class="n">default_arg</span> <span class="o">=</span> <span class="n">_get_default_arg</span><span class="p">(</span><span class="n">argspec</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="n">argspec</span><span class="o">.</span><span class="n">defaults</span><span class="p">,</span> <span class="n">arg_index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">default_arg</span><span class="o">.</span><span class="n">has_default</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">default_arg</span><span class="o">.</span><span class="n">default_value</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
        <span class="n">arg_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">(</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">method</span><span class="o">.</span><span class="n">__name__</span><span class="p">,</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>

<span class="k">for</span> <span class="n">_nn_name</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getmembers</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="p">,</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isfunction</span><span class="p">):</span>
 	<span class="n">_layer_name</span> <span class="o">=</span> <span class="n">_nn_name</span> <span class="o">+</span> <span class="s2">&quot;_layer&quot;</span>
 	<span class="n">_map_name</span> <span class="o">=</span> <span class="s2">&quot;map_&quot;</span> <span class="o">+</span> <span class="n">_nn_name</span>
 	<span class="n">_f_signature</span> <span class="o">=</span> <span class="n">_get_method_sig</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
 	<span class="n">_f_docs</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getdoc</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>


 	<span class="k">exec</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">@tb._immutable</span>
<span class="s2">def {1}(builder, size, *args, **kwargs):</span>
<span class="s2">	</span><span class="se">\&quot;\&quot;\&quot;</span><span class="s2"></span>
<span class="s2">THIS METHOD IS AUTOMATICALLY GENERATED</span>

<span class="s2">**@_immutable**</span>

<span class="s2">It connects the current tensor to a layer whos output function is `tf.nn.{0}`. The keyword parameters `weights_name`, `bias` and `bias_name` are set to defaults if they are not present in \*\*kwargs. Any additional positional (\*args) and keyword arguments (\*\*kwargs) will be forwarded to `tf.nn.{0}`. </span>

<span class="s2">**Note:** </span>

<span class="s2">Its expected that tf.nn.{0} can receive `builder.tensor` as a first parameter.</span>

<span class="s2">** TensorFlow documentation for `tf.nn.{0}`**</span>

<span class="s2">	def {2}</span>
<span class="s2">	</span>
<span class="s2">{3}</span>

<span class="s2">	</span><span class="se">\&quot;\&quot;\&quot;</span><span class="s2"></span>
<span class="s2">	name = {1} if &quot;name&quot; not in kwargs else kwargs[&quot;name&quot;]</span>
<span class="s2">	weights_name = None if &quot;weights_name&quot; not in kwargs else kwargs[&quot;weights_name&quot;]</span>
<span class="s2">	bias = True if &quot;bias&quot; not in kwargs else kwargs[&quot;bias&quot;]</span>
<span class="s2">	bias_name = None if &quot;bias_name&quot; not in kwargs else kwargs[&quot;bias_name&quot;]</span>

<span class="s2">	if &quot;weights_name&quot; in kwargs:</span>
<span class="s2">		del kwargs[&quot;weights_name&quot;]</span>
<span class="s2">	if &quot;bias&quot; in kwargs:</span>
<span class="s2">		del kwargs[&quot;bias&quot;]</span>
<span class="s2">	if &quot;bias_name&quot; in kwargs:</span>
<span class="s2">		del kwargs[&quot;bias_name&quot;]</span>

<span class="s2">	return (</span>
<span class="s2">		builder</span>
<span class="s2">		.connect_layer(size, weights_name=weights_name, bias=bias, bias_name=bias_name)</span>
<span class="s2">		.map(tf.nn.{0}, *args, **kwargs)</span>
<span class="s2">	)</span>

<span class="s2">_patches.append(</span><span class="se">\&quot;</span><span class="s2">tb.Builder.{1} = {1}</span><span class="se">\&quot;</span><span class="s2">)</span>

<span class="s2"> 	&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">_nn_name</span><span class="p">,</span> <span class="n">_layer_name</span><span class="p">,</span> <span class="n">_f_signature</span><span class="p">,</span> <span class="n">_f_docs</span><span class="p">))</span>

 	<span class="k">exec</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">@tb._immutable</span>
<span class="s2">def {1}(builder, *args, **kwargs):</span>
<span class="s2">	</span><span class="se">\&quot;\&quot;\&quot;</span><span class="s2"></span>
<span class="s2">THIS METHOD IS AUTOMATICALLY GENERATED</span>

<span class="s2">**@_immutable**</span>

<span class="s2">It maps `tf.nn.{0}` over the current tensor. All positional (\*args) and keyword arguments (\*\*kwargs) are forwarded to `tf.nn.{0}`.</span>

<span class="s2">**Note:** </span>

<span class="s2">Its expected that tf.nn.{0} can receive `builder.tensor` as a first parameter.</span>

<span class="s2">** TensorFlow documentation for `tf.nn.{0}` **</span>

<span class="s2">	def {2}</span>

<span class="s2">{3}</span>
<span class="s2">	</span><span class="se">\&quot;\&quot;\&quot;</span><span class="s2"></span>
<span class="s2">	return (</span>
<span class="s2">		builder</span>
<span class="s2">		.map(tf.nn.{0}, *args, **kwargs)</span>
<span class="s2">	)</span>

<span class="s2">_patches.append(</span><span class="se">\&quot;</span><span class="s2">tb.Builder.{1} = {1}</span><span class="se">\&quot;</span><span class="s2">)</span>

<span class="s2"> 	&quot;&quot;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">_nn_name</span><span class="p">,</span> <span class="n">_map_name</span><span class="p">,</span> <span class="n">_f_signature</span><span class="p">,</span> <span class="n">_f_docs</span><span class="p">))</span>
</pre></div>

  </div>

  </header>

  <section id="section-items">

    <h2 class="section-title" id="header-functions">Functions</h2>
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.all_candidate_sampler_layer">
    <p>def <span class="ident">all_candidate_sampler_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.all_candidate_sampler</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.all_candidate_sampler</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.all_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.all_candidate_sampler</code></strong></p>
<div class="codehilite"><pre><span></span>    def all_candidate_sampler(true_classes, num_true, num_sampled, unique, seed=None, name=None)
</pre></div>


<p>Generate the set of all classes.</p>
<p>Deterministically generates and returns the set of all possible classes.
For testing purposes.  There is no need to use this, since you might as
well use full softmax or full logistic regression.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of possible classes.
  unique: A <code>bool</code>. Ignored.
    unique.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    This operation deterministically returns the entire range
    <code>[0, num_sampled]</code>.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>. All returned values are 1.0.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>. All returned values are 1.0.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.avg_pool_layer">
    <p>def <span class="ident">avg_pool_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.avg_pool</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.avg_pool</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.avg_pool can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.avg_pool</code></strong></p>
<div class="codehilite"><pre><span></span>    def avg_pool(value, ksize, strides, padding, data_format=&quot;NHWC&quot;, name=None)
</pre></div>


<p>Performs the average pooling on the input.</p>
<p>Each entry in <code>output</code> is the mean of the corresponding size <code>ksize</code>
window in <code>value</code>.</p>
<p>Args:
  value: A 4-D <code>Tensor</code> of shape <code>[batch, height, width, channels]</code> and type
    <code>float32</code>, <code>float64</code>, <code>qint8</code>, <code>quint8</code>, or <code>qint32</code>.
  ksize: A list of ints that has length &gt;= 4.
    The size of the window for each dimension of the input tensor.
  strides: A list of ints that has length &gt;= 4.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm.
  data_format: A string. 'NHWC' and 'NCHW' are supported.
  name: Optional name for the operation.</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>value</code>.  The average pooled output tensor.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.batch_norm_with_global_normalization_layer">
    <p>def <span class="ident">batch_norm_with_global_normalization_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.batch_norm_with_global_normalization</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.batch_norm_with_global_normalization</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.batch_norm_with_global_normalization can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.batch_norm_with_global_normalization</code></strong></p>
<div class="codehilite"><pre><span></span>    def batch_norm_with_global_normalization(t, m, v, beta, gamma, variance_epsilon, scale_after_normalization, name=None)
</pre></div>


<p>Batch normalization.</p>
<p>This op is deprecated. See <code>tf.nn.batch_normalization</code>.</p>
<p>Args:
  t: A 4D input Tensor.
  m: A 1D mean Tensor with size matching the last dimension of t.
    This is the first output from tf.nn.moments,
    or a saved moving average thereof.
  v: A 1D variance Tensor with size matching the last dimension of t.
    This is the second output from tf.nn.moments,
    or a saved moving average thereof.
  beta: A 1D beta Tensor with size matching the last dimension of t.
    An offset to be added to the normalized tensor.
  gamma: A 1D gamma Tensor with size matching the last dimension of t.
    If "scale_after_normalization" is true, this tensor will be multiplied
    with the normalized tensor.
  variance_epsilon: A small float number to avoid dividing by 0.
  scale_after_normalization: A bool indicating whether the resulted tensor
    needs to be multiplied with gamma.
  name: A name for this operation (optional).</p>
<p>Returns:
   A batch-normalized <code>t</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.batch_normalization_layer">
    <p>def <span class="ident">batch_normalization_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.batch_normalization</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.batch_normalization</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.batch_normalization can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.batch_normalization</code></strong></p>
<div class="codehilite"><pre><span></span>    def batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None)
</pre></div>


<p>Batch normalization.</p>
<p>As described in http://arxiv.org/abs/1502.03167.
Normalizes a tensor by <code>mean</code> and <code>variance</code>, and applies (optionally) a
<code>scale</code> (\gamma) to it, as well as an <code>offest</code> (eta):</p>
<p>(rac{\gamma(x-\mu)}{\sigma}+eta)</p>
<p><code>mean</code>, <code>variance</code>, <code>offset</code> and <code>scale</code> are all expected to be of one of two
shapes:
  * In all generality, they can have the same number of dimensions as the
    input <code>x</code>, with identical sizes as <code>x</code> for the dimensions that are not
    normalized over (the 'depth' dimension(s)), and dimension 1 for the
    others which are being normalized over.
    <code>mean</code> and <code>variance</code> in this case would typically be the outputs of
    <code>tf.nn.moments(..., keep_dims=True)</code> during training, or running averages
    thereof during inference.
  * In the common case where the 'depth' dimension is the last dimension in
    the input tensor <code>x</code>, they may be one dimensional tensors of the same
    size as the 'depth' dimension.
    This is the case for example for the common <code>[batch, depth]</code> layout of
    fully-connected layers, and <code>[batch, height, width, depth]</code> for
    convolutions.
    <code>mean</code> and <code>variance</code> in this case would typically be the outputs of
    <code>tf.nn.moments(..., keep_dims=False)</code> during training, or running averages
    thereof during inference.</p>
<p>Args:
  x: Input <code>Tensor</code> of arbitrary dimensionality.
  mean: A mean <code>Tensor</code>.
  variance: A variance <code>Tensor</code>.
  offset: An offset <code>Tensor</code>, often denoted (eta) in equations, or
    None. If present, will be added to the normalized tensor.
  scale: A scale <code>Tensor</code>, often denoted (\gamma) in equations, or
    <code>None</code>. If present, the scale is applied to the normalized tensor.
  variance_epsilon: A small float number to avoid dividing by 0.
  name: A name for this operation (optional).</p>
<p>Returns:
  the normalized, scaled, offset tensor.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.bias_add_grad_layer">
    <p>def <span class="ident">bias_add_grad_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.bias_add_grad</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.bias_add_grad</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.bias_add_grad can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.bias_add_grad</code></strong></p>
<div class="codehilite"><pre><span></span>    def bias_add_grad(out_backprop, data_format=None, name=None)
</pre></div>


<p>The backward operation for "BiasAdd" on the "bias" tensor.</p>
<p>It accumulates all the values from out_backprop into the feature dimension.
For NHWC data format, the feature dimension is the last. For NCHW data format,
the feature dimension is the third-to-last.</p>
<p>Args:
  out_backprop: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.
    Any number of dimensions.
  data_format: An optional <code>string</code> from: <code>"NHWC", "NCHW"</code>. Defaults to <code>"NHWC"</code>.
    Specify the data format of the input and output data. With the
    default format "NHWC", the bias tensor will be added to the last dimension
    of the value tensor.
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
    The tensor will be added to "in_channels", the third-to-the-last
        dimension.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>out_backprop</code>.
  1-D with size the feature dimension of <code>out_backprop</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.bias_add_layer">
    <p>def <span class="ident">bias_add_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.bias_add</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.bias_add</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.bias_add can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.bias_add</code></strong></p>
<div class="codehilite"><pre><span></span>    def bias_add(value, bias, data_format=None, name=None)
</pre></div>


<p>Adds <code>bias</code> to <code>value</code>.</p>
<p>This is (mostly) a special case of <code>tf.add</code> where <code>bias</code> is restricted to 1-D.
Broadcasting is supported, so <code>value</code> may have any number of dimensions.
Unlike <code>tf.add</code>, the type of <code>bias</code> is allowed to differ from <code>value</code> in the
case where both types are quantized.</p>
<p>Args:
  value: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>,
    <code>int16</code>, <code>int8</code>, or <code>complex64</code>.
  bias: A 1-D <code>Tensor</code> with size matching the last dimension of <code>value</code>.
    Must be the same type as <code>value</code> unless <code>value</code> is a quantized type,
    in which case a different quantized type may be used.
  data_format: A string. 'NHWC' and 'NCHW' are supported.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>value</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.bias_add_v1_layer">
    <p>def <span class="ident">bias_add_v1_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.bias_add_v1</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.bias_add_v1</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.bias_add_v1 can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.bias_add_v1</code></strong></p>
<div class="codehilite"><pre><span></span>    def bias_add_v1(value, bias, name=None)
</pre></div>


<p>Adds <code>bias</code> to <code>value</code>.</p>
<p>This is a deprecated version of bias_add and will soon to be removed.</p>
<p>This is (mostly) a special case of <code>tf.add</code> where <code>bias</code> is restricted to 1-D.
Broadcasting is supported, so <code>value</code> may have any number of dimensions.
Unlike <code>tf.add</code>, the type of <code>bias</code> is allowed to differ from <code>value</code> in the
case where both types are quantized.</p>
<p>Args:
  value: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>,
    <code>int16</code>, <code>int8</code>, or <code>complex64</code>.
  bias: A 1-D <code>Tensor</code> with size matching the last dimension of <code>value</code>.
    Must be the same type as <code>value</code> unless <code>value</code> is a quantized type,
    in which case a different quantized type may be used.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>value</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.bidirectional_rnn_layer">
    <p>def <span class="ident">bidirectional_rnn_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.bidirectional_rnn</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.bidirectional_rnn</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.bidirectional_rnn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.bidirectional_rnn</code></strong></p>
<div class="codehilite"><pre><span></span>    def bidirectional_rnn(cell_fw, cell_bw, inputs, initial_state_fw=None, initial_state_bw=None, dtype=None, sequence_length=None, scope=None)
</pre></div>


<p>Creates a bidirectional recurrent neural network.</p>
<p>Similar to the unidirectional case above (rnn) but takes input and builds
independent forward and backward RNNs with the final forward and backward
outputs depth-concatenated, such that the output will have the format
[time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of
forward and backward cell must match. The initial state for both directions
is zero by default (but can be set optionally) and no intermediate states are
ever returned -- the network is fully unrolled for the given (passed in)
length(s) of the sequence(s) or completely unrolled if length(s) is not given.</p>
<p>Args:
  cell_fw: An instance of RNNCell, to be used for forward direction.
  cell_bw: An instance of RNNCell, to be used for backward direction.
  inputs: A length T list of inputs, each a tensor of shape
    [batch_size, cell.input_size].
  initial_state_fw: (optional) An initial state for the forward RNN.
    This must be a tensor of appropriate type and shape
    [batch_size x cell.state_size].
  initial_state_bw: (optional) Same as for initial_state_fw.
  dtype: (optional) The data type for the initial state.  Required if either
    of the initial states are not provided.
  sequence_length: (optional) An int32/int64 vector, size [batch_size],
    containing the actual lengths for each of the sequences.
  scope: VariableScope for the created subgraph; defaults to "BiRNN"</p>
<p>Returns:
  A tuple (outputs, output_state_fw, output_state_bw) where:
    outputs is a length T list of outputs (one for each input), which
    are depth-concatenated forward and backward outputs
    output_state_fw is the final state of the forward rnn
    output_state_bw is the final state of the backward rnn</p>
<p>Raises:
  TypeError: If "cell_fw" or "cell_bw" is not an instance of RNNCell.
  ValueError: If inputs is None or an empty list.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.compute_accidental_hits_layer">
    <p>def <span class="ident">compute_accidental_hits_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.compute_accidental_hits</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.compute_accidental_hits</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.compute_accidental_hits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.compute_accidental_hits</code></strong></p>
<div class="codehilite"><pre><span></span>    def compute_accidental_hits(true_classes, sampled_candidates, num_true, seed=None, name=None)
</pre></div>


<p>Compute the position ids in <code>sampled_candidates</code> matching <code>true_classes</code>.</p>
<p>In Candidate Sampling, this operation facilitates virtually removing
sampled classes which happen to match target classes.  This is done
in Sampled Softmax and Sampled Logistic.</p>
<p>See our <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">Candidate Sampling Algorithms
Reference</a>.</p>
<p>We presuppose that the <code>sampled_candidates</code> are unique.</p>
<p>We call it an 'accidental hit' when one of the target classes
matches one of the sampled classes.  This operation reports
accidental hits as triples <code>(index, id, weight)</code>, where <code>index</code>
represents the row number in <code>true_classes</code>, <code>id</code> represents the
position in <code>sampled_candidates</code>, and weight is <code>-FLOAT_MAX</code>.</p>
<p>The result of this op should be passed through a <code>sparse_to_dense</code>
operation, then added to the logits of the sampled classes. This
removes the contradictory effect of accidentally sampling the true
target classes as noise classes for the same example.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled_candidates output of CandidateSampler.
  num_true: An <code>int</code>.  The number of target classes per training example.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  indices: A <code>Tensor</code> of type <code>int32</code> and shape <code>[num_accidental_hits]</code>.
    Values indicate rows in <code>true_classes</code>.
  ids: A <code>Tensor</code> of type <code>int64</code> and shape <code>[num_accidental_hits]</code>.
    Values indicate positions in <code>sampled_candidates</code>.
  weights: A <code>Tensor</code> of type <code>float</code> and shape <code>[num_accidental_hits]</code>.
    Each value is <code>-FLOAT_MAX</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.conv2d_backprop_filter_layer">
    <p>def <span class="ident">conv2d_backprop_filter_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.conv2d_backprop_filter</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.conv2d_backprop_filter</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.conv2d_backprop_filter can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.conv2d_backprop_filter</code></strong></p>
<div class="codehilite"><pre><span></span>    def conv2d_backprop_filter(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)
</pre></div>


<p>Computes the gradients of convolution with respect to the filter.</p>
<p>Args:
  input: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.
  filter_sizes: A <code>Tensor</code> of type <code>int32</code>.
    An integer vector representing the tensor shape of <code>filter</code>,
    where <code>filter</code> is a 4-D
    <code>[filter_height, filter_width, in_channels, out_channels]</code> tensor.
  out_backprop: A <code>Tensor</code>. Must have the same type as <code>input</code>.
    4-D with shape <code>[batch, out_height, out_width, out_channels]</code>.
    Gradients w.r.t. the output of the convolution.
  strides: A list of <code>ints</code>.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional <code>bool</code>. Defaults to <code>True</code>.
  data_format: An optional <code>string</code> from: <code>"NHWC", "NCHW"</code>. Defaults to <code>"NHWC"</code>.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>input</code>. 4-D with shape
  <code>[filter_height, filter_width, in_channels, out_channels]</code>.  Gradient w.r.t.
  the <code>filter</code> input of the convolution.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.conv2d_backprop_input_layer">
    <p>def <span class="ident">conv2d_backprop_input_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.conv2d_backprop_input</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.conv2d_backprop_input</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.conv2d_backprop_input can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.conv2d_backprop_input</code></strong></p>
<div class="codehilite"><pre><span></span>    def conv2d_backprop_input(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)
</pre></div>


<p>Computes the gradients of convolution with respect to the input.</p>
<p>Args:
  input_sizes: A <code>Tensor</code> of type <code>int32</code>.
    An integer vector representing the shape of <code>input</code>,
    where <code>input</code> is a 4-D <code>[batch, height, width, channels]</code> tensor.
  filter: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    4-D with shape
    <code>[filter_height, filter_width, in_channels, out_channels]</code>.
  out_backprop: A <code>Tensor</code>. Must have the same type as <code>filter</code>.
    4-D with shape <code>[batch, out_height, out_width, out_channels]</code>.
    Gradients w.r.t. the output of the convolution.
  strides: A list of <code>ints</code>.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional <code>bool</code>. Defaults to <code>True</code>.
  data_format: An optional <code>string</code> from: <code>"NHWC", "NCHW"</code>. Defaults to <code>"NHWC"</code>.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>filter</code>.
  4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.  Gradient
  w.r.t. the input of the convolution.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.conv2d_layer">
    <p>def <span class="ident">conv2d_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.conv2d</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.conv2d</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.conv2d can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.conv2d</code></strong></p>
<div class="codehilite"><pre><span></span>    def conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)
</pre></div>


<p>Computes a 2-D convolution given 4-D <code>input</code> and <code>filter</code> tensors.</p>
<p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code>
and a filter / kernel tensor of shape
<code>[filter_height, filter_width, in_channels, out_channels]</code>, this op
performs the following:</p>
<ol>
<li>Flattens the filter to a 2-D matrix with shape
   <code>[filter_height * filter_width * in_channels, output_channels]</code>.</li>
<li>Extracts image patches from the input tensor to form a <em>virtual</em>
   tensor of shape <code>[batch, out_height, out_width,
   filter_height * filter_width * in_channels]</code>.</li>
<li>For each patch, right-multiplies the filter matrix and the image patch
   vector.</li>
</ol>
<p>In detail, with the default NHWC format,</p>
<div class="codehilite"><pre><span></span>output[b, i, j, k] =
    sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *
                    filter[di, dj, q, k]
</pre></div>


<p>Must have <code>strides[0] = strides[3] = 1</code>.  For the most common case of the same
horizontal and vertices strides, <code>strides = [1, stride, stride, 1]</code>.</p>
<p>Args:
  input: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
  filter: A <code>Tensor</code>. Must have the same type as <code>input</code>.
  strides: A list of <code>ints</code>.
    1-D of length 4.  The stride of the sliding window for each dimension
    of <code>input</code>. Must be in the same order as the dimension specified with format.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional <code>bool</code>. Defaults to <code>True</code>.
  data_format: An optional <code>string</code> from: <code>"NHWC", "NCHW"</code>. Defaults to <code>"NHWC"</code>.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>input</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.conv2d_transpose_layer">
    <p>def <span class="ident">conv2d_transpose_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.conv2d_transpose</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.conv2d_transpose</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.conv2d_transpose can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.conv2d_transpose</code></strong></p>
<div class="codehilite"><pre><span></span>    def conv2d_transpose(value, filter, output_shape, strides, padding=&quot;SAME&quot;, name=None)
</pre></div>


<p>The transpose of <code>conv2d</code>.</p>
<p>This operation is sometimes called "deconvolution" after (Deconvolutional
Networks)[http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf], but is
actually the transpose (gradient) of <code>conv2d</code> rather than an actual
deconvolution.</p>
<p>Args:
  value: A 4-D <code>Tensor</code> of type <code>float</code> and shape
    <code>[batch, height, width, in_channels]</code>.
  filter: A 4-D <code>Tensor</code> with the same type as <code>value</code> and shape
    <code>[height, width, output_channels, in_channels]</code>.  <code>filter</code>'s
    <code>in_channels</code> dimension must match that of <code>value</code>.
  output_shape: A 1-D <code>Tensor</code> representing the output shape of the
    deconvolution op.
  strides: A list of ints. The stride of the sliding window for each
    dimension of the input tensor.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm.
  name: Optional name for the returned tensor.</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>value</code>.</p>
<p>Raises:
  ValueError: If input/output depth does not match <code>filter</code>'s shape, or if
    padding is other than <code>'VALID'</code> or <code>'SAME'</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.depthwise_conv2d_layer">
    <p>def <span class="ident">depthwise_conv2d_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.depthwise_conv2d</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.depthwise_conv2d</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.depthwise_conv2d can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.depthwise_conv2d</code></strong></p>
<div class="codehilite"><pre><span></span>    def depthwise_conv2d(input, filter, strides, padding, name=None)
</pre></div>


<p>Depthwise 2-D convolution.</p>
<p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code>
and a filter tensor of shape
<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>
containing <code>in_channels</code> convolutional filters of depth 1, <code>depthwise_conv2d</code>
applies a different filter to each input channel (expanding from 1 channel
to <code>channel_multiplier</code> channels for each), then concatenates the results
together.  The output has <code>in_channels * channel_multiplier</code> channels.</p>
<p>In detail,</p>
<div class="codehilite"><pre><span></span>output[b, i, j, k * channel_multiplier + q] =
    sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *
                 filter[di, dj, k, q]
</pre></div>


<p>Must have <code>strides[0] = strides[3] = 1</code>.  For the most common case of the
same horizontal and vertical strides, <code>strides = [1, stride, stride, 1]</code>.</p>
<p>Args:
  input: 4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.
  filter: 4-D with shape
    <code>[filter_height, filter_width, in_channels, channel_multiplier]</code>.
  strides: 1-D of size 4.  The stride of the sliding window for each
    dimension of <code>input</code>.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>.  The padding algorithm.
  name: A name for this operation (optional).</p>
<p>Returns:
  A 4-D <code>Tensor</code> of shape
  <code>[batch, out_height, out_width, in_channels * channel_multiplier].</code></p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.depthwise_conv2d_native_backprop_filter_layer">
    <p>def <span class="ident">depthwise_conv2d_native_backprop_filter_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.depthwise_conv2d_native_backprop_filter</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.depthwise_conv2d_native_backprop_filter</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.depthwise_conv2d_native_backprop_filter can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.depthwise_conv2d_native_backprop_filter</code></strong></p>
<div class="codehilite"><pre><span></span>    def depthwise_conv2d_native_backprop_filter(input, filter_sizes, out_backprop, strides, padding, name=None)
</pre></div>


<p>Computes the gradients of depthwise convolution with respect to the filter.</p>
<p>Args:
  input: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.
  filter_sizes: A <code>Tensor</code> of type <code>int32</code>.
    An integer vector representing the tensor shape of <code>filter</code>,
    where <code>filter</code> is a 4-D
    <code>[filter_height, filter_width, in_channels, depthwise_multiplier]</code> tensor.
  out_backprop: A <code>Tensor</code>. Must have the same type as <code>input</code>.
    4-D with shape <code>[batch, out_height, out_width, out_channels]</code>.
    Gradients w.r.t. the output of the convolution.
  strides: A list of <code>ints</code>.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>input</code>. 4-D with shape
  <code>[filter_height, filter_width, in_channels, out_channels]</code>.  Gradient w.r.t.
  the <code>filter</code> input of the convolution.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.depthwise_conv2d_native_backprop_input_layer">
    <p>def <span class="ident">depthwise_conv2d_native_backprop_input_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.depthwise_conv2d_native_backprop_input</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.depthwise_conv2d_native_backprop_input</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.depthwise_conv2d_native_backprop_input can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.depthwise_conv2d_native_backprop_input</code></strong></p>
<div class="codehilite"><pre><span></span>    def depthwise_conv2d_native_backprop_input(input_sizes, filter, out_backprop, strides, padding, name=None)
</pre></div>


<p>Computes the gradients of depthwise convolution with respect to the input.</p>
<p>Args:
  input_sizes: A <code>Tensor</code> of type <code>int32</code>.
    An integer vector representing the shape of <code>input</code>,
    where <code>input</code> is a 4-D <code>[batch, height, width, channels]</code> tensor.
  filter: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    4-D with shape
    <code>[filter_height, filter_width, in_channels, depthwise_multiplier]</code>.
  out_backprop: A <code>Tensor</code>. Must have the same type as <code>filter</code>.
    4-D with shape <code>[batch, out_height, out_width, out_channels]</code>.
    Gradients w.r.t. the output of the convolution.
  strides: A list of <code>ints</code>.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>filter</code>.
  4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.  Gradient
  w.r.t. the input of the convolution.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.depthwise_conv2d_native_layer">
    <p>def <span class="ident">depthwise_conv2d_native_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.depthwise_conv2d_native</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.depthwise_conv2d_native</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.depthwise_conv2d_native can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.depthwise_conv2d_native</code></strong></p>
<div class="codehilite"><pre><span></span>    def depthwise_conv2d_native(input, filter, strides, padding, name=None)
</pre></div>


<p>Computes a 2-D depthwise convolution given 4-D <code>input</code> and <code>filter</code> tensors.</p>
<p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code>
and a filter / kernel tensor of shape
<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>, containing
<code>in_channels</code> convolutional filters of depth 1, <code>depthwise_conv2d</code> applies
a different filter to each input channel (expanding from 1 channel to
<code>channel_multiplier</code> channels for each), then concatenates the results
together. Thus, the output has <code>in_channels * channel_multiplier</code> channels.</p>
<p>for k in 0..in_channels-1
  for q in 0..channel_multiplier-1
    output[b, i, j, k * channel_multiplier + q] =
      sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *
                        filter[di, dj, k, q]</p>
<p>Must have <code>strides[0] = strides[3] = 1</code>.  For the most common case of the same
horizontal and vertices strides, <code>strides = [1, stride, stride, 1]</code>.</p>
<p>Args:
  input: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
  filter: A <code>Tensor</code>. Must have the same type as <code>input</code>.
  strides: A list of <code>ints</code>.
    1-D of length 4.  The stride of the sliding window for each dimension
    of <code>input</code>.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>input</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.dropout_layer">
    <p>def <span class="ident">dropout_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.dropout</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.dropout</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.dropout can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.dropout</code></strong></p>
<div class="codehilite"><pre><span></span>    def dropout(x, keep_prob, noise_shape=None, seed=None, name=None)
</pre></div>


<p>Computes dropout.</p>
<p>With probability <code>keep_prob</code>, outputs the input element scaled up by
<code>1 / keep_prob</code>, otherwise outputs <code>0</code>.  The scaling is so that the expected
sum is unchanged.</p>
<p>By default, each element is kept or dropped independently.  If <code>noise_shape</code>
is specified, it must be
<a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">broadcastable</a>
to the shape of <code>x</code>, and only dimensions with <code>noise_shape[i] == shape(x)[i]</code>
will make independent decisions.  For example, if <code>shape(x) = [k, l, m, n]</code>
and <code>noise_shape = [k, 1, 1, n]</code>, each batch and channel component will be
kept independently and each row and column will be kept or not kept together.</p>
<p>Args:
  x: A tensor.
  keep_prob: A scalar <code>Tensor</code> with the same type as x. The probability
    that each element is kept.
  noise_shape: A 1-D <code>Tensor</code> of type <code>int32</code>, representing the
    shape for randomly generated keep/drop flags.
  seed: A Python integer. Used to create random seeds. See
    <a href="../../api_docs/python/constant_op.md#set_random_seed"><code>set_random_seed</code></a>
    for behavior.
  name: A name for this operation (optional).</p>
<p>Returns:
  A Tensor of the same shape of <code>x</code>.</p>
<p>Raises:
  ValueError: If <code>keep_prob</code> is not in <code>(0, 1]</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.dynamic_rnn_layer">
    <p>def <span class="ident">dynamic_rnn_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.dynamic_rnn</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.dynamic_rnn</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.dynamic_rnn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.dynamic_rnn</code></strong></p>
<div class="codehilite"><pre><span></span>    def dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None)
</pre></div>


<p>Creates a recurrent neural network specified by RNNCell "cell".</p>
<p>This function is functionally identical to the function <code>rnn</code> above, but
performs fully dynamic unrolling of <code>inputs</code>.</p>
<p>Unlike <code>rnn</code>, the input <code>inputs</code> is not a Python list of <code>Tensors</code>.  Instead,
it is a single <code>Tensor</code> where the maximum time is either the first or second
dimension (see the parameter <code>time_major</code>).  The corresponding output is
a single <code>Tensor</code> having the same number of time steps and batch size.</p>
<p>The parameter <code>sequence_length</code> is required and dynamic calculation is
automatically performed.</p>
<p>Args:
  cell: An instance of RNNCell.
  inputs: The RNN inputs.
    If time_major == False (default), this must be a tensor of shape:
      <code>[batch_size, max_time, cell.input_size]</code>.
    If time_major == True, this must be a tensor of shape:
      <code>[max_time, batch_size, cell.input_size]</code>.
  sequence_length: (optional) An int32/int64 vector sized <code>[batch_size]</code>.
  initial_state: (optional) An initial state for the RNN.  This must be
    a tensor of appropriate type and shape <code>[batch_size x cell.state_size]</code>.
  dtype: (optional) The data type for the initial state.  Required if
    initial_state is not provided.
  parallel_iterations: (Default: 32).  The number of iterations to run in
    parallel.  Those operations which do not have any temporal dependency
    and can be run in parallel, will be.  This parameter trades off
    time for space.  Values &gt;&gt; 1 use more memory but take less time,
    while smaller values use less memory but computations take longer.
  swap_memory: Swap the tensors produced in forward inference but needed
    for back prop from GPU to CPU.
  time_major: The shape format of the <code>inputs</code> and <code>outputs</code> Tensors.
    If true, these <code>Tensors</code> must be shaped <code>[max_time, batch_size, depth]</code>.
    If false, these <code>Tensors</code> must be shaped <code>[batch_size, max_time, depth]</code>.
    Using time_major = False is a bit more efficient because it avoids
    transposes at the beginning and end of the RNN calculation.  However,
    most TensorFlow data is batch-major, so by default this function
    accepts input and emits output in batch-major form.
  scope: VariableScope for the created subgraph; defaults to "RNN".</p>
<p>Returns:
  A pair (outputs, state) where:
    outputs: The RNN output <code>Tensor</code>.
      If time_major == False (default), this will be a <code>Tensor</code> shaped:
        <code>[batch_size, max_time, cell.output_size]</code>.
      If time_major == True, this will be a <code>Tensor</code> shaped:
        <code>[max_time, batch_size, cell.output_size]</code>.
    state: The final state, shaped:
      <code>[batch_size, cell.state_size]</code>.</p>
<p>Raises:
  TypeError: If "cell" is not an instance of RNNCell.
  ValueError: If inputs is None or an empty list.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.elu_layer">
    <p>def <span class="ident">elu_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.elu</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.elu</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.elu can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.elu</code></strong></p>
<div class="codehilite"><pre><span></span>    def elu(features, name=None)
</pre></div>


<p>Computes exponential linear: <code>exp(features) - 1</code> if &lt; 0, <code>features</code> otherwise.</p>
<p>See <a href="http://arxiv.org/abs/1511.07289">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
</a></p>
<p>Args:
  features: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.embedding_lookup_layer">
    <p>def <span class="ident">embedding_lookup_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.embedding_lookup</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.embedding_lookup</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.embedding_lookup can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.embedding_lookup</code></strong></p>
<div class="codehilite"><pre><span></span>    def embedding_lookup(params, ids, partition_strategy=&quot;mod&quot;, name=None, validate_indices=True)
</pre></div>


<p>Looks up <code>ids</code> in a list of embedding tensors.</p>
<p>This function is used to perform parallel lookups on the list of
tensors in <code>params</code>.  It is a generalization of
<a href="../../api_docs/python/array_ops.md#gather"><code>tf.gather()</code></a>, where <code>params</code> is
interpreted as a partition of a larger embedding tensor.</p>
<p>If <code>len(params) &gt; 1</code>, each element <code>id</code> of <code>ids</code> is partitioned between
the elements of <code>params</code> according to the <code>partition_strategy</code>.
In all strategies, if the id space does not evenly divide the number of
partitions, each of the first <code>(max_id + 1) % len(params)</code> partitions will
be assigned one more id.</p>
<p>If <code>partition_strategy</code> is <code>"mod"</code>, we assign each id to partition
<code>p = id % len(params)</code>. For instance,
13 ids are split across 5 partitions as:
<code>[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]</code></p>
<p>If <code>partition_strategy</code> is <code>"div"</code>, we assign ids to partitions in a
contiguous manner. In this case, 13 ids are split across 5 partitions as:
<code>[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</code></p>
<p>The results of the lookup are concatenated into a dense
tensor. The returned tensor has shape <code>shape(ids) + shape(params)[1:]</code>.</p>
<p>Args:
  params: A list of tensors with the same type and which can be concatenated
    along dimension 0. Each <code>Tensor</code> must be appropriately sized for the given
    <code>partition_strategy</code>.
  ids: A <code>Tensor</code> with type <code>int32</code> or <code>int64</code> containing the ids to be looked
    up in <code>params</code>.
  partition_strategy: A string specifying the partitioning strategy, relevant
    if <code>len(params) &gt; 1</code>. Currently <code>"div"</code> and <code>"mod"</code> are supported. Default
    is <code>"mod"</code>.
  name: A name for the operation (optional).
  validate_indices: Whether or not to validate gather indices.</p>
<p>Returns:
  A <code>Tensor</code> with the same type as the tensors in <code>params</code>.</p>
<p>Raises:
  ValueError: If <code>params</code> is empty.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.embedding_lookup_sparse_layer">
    <p>def <span class="ident">embedding_lookup_sparse_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.embedding_lookup_sparse</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.embedding_lookup_sparse</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.embedding_lookup_sparse can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.embedding_lookup_sparse</code></strong></p>
<div class="codehilite"><pre><span></span>    def embedding_lookup_sparse(params, sp_ids, sp_weights, partition_strategy=&quot;mod&quot;, name=None, combiner=&quot;mean&quot;)
</pre></div>


<p>Computes embeddings for the given ids and weights.</p>
<p>This op assumes that there is at least one id for each row in the dense tensor
represented by sp_ids (i.e. there are no rows with empty features), and that
all the indices of sp_ids are in canonical row-major order.</p>
<p>It also assumes that all id values lie in the range [0, p0), where p0
is the sum of the size of params along dimension 0.</p>
<p>Args:
  params: A single tensor representing the complete embedding tensor,
    or a list of P tensors all of same shape except for the first dimension,
    representing sharded embedding tensors.
  sp_ids: N x M SparseTensor of int64 ids (typically from FeatureValueToId),
    where N is typically batch size and M is arbitrary.
  sp_weights: either a SparseTensor of float / double weights, or None to
    indicate all weights should be taken to be 1. If specified, sp_weights
    must have exactly the same shape and indices as sp_ids.
  partition_strategy: A string specifying the partitioning strategy, relevant
    if <code>len(params) &gt; 1</code>. Currently <code>"div"</code> and <code>"mod"</code> are supported. Default
    is <code>"mod"</code>. See <code>tf.nn.embedding_lookup</code> for more details.
  name: Optional name for the op.
  combiner: A string specifying the reduction op. Currently "mean", "sqrtn"
    and "sum" are supported.
    "sum" computes the weighted sum of the embedding results for each row.
    "mean" is the weighted sum divided by the total weight.
    "sqrtn" is the weighted sum divided by the square root of the sum of the
    squares of the weights.</p>
<p>Returns:
  A dense tensor representing the combined embeddings for the
  sparse ids. For each row in the dense tensor represented by sp_ids, the op
  looks up the embeddings for all ids in that row, multiplies them by the
  corresponding weight, and combines these embeddings as specified.</p>
<p>In other words, if
    shape(combined params) = [p0, p1, ..., pm]
  and
    shape(sp_ids) = shape(sp_weights) = [d0, d1, ..., dn]
  then
    shape(output) = [d0, d1, ..., dn-1, p1, ..., pm].</p>
<p>For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are</p>
<div class="codehilite"><pre><span></span>[0, 0]: id 1, weight 2.0
[0, 1]: id 3, weight 0.5
[1, 0]: id 0, weight 1.0
[2, 3]: id 1, weight 3.0
</pre></div>


<p>with combiner="mean", then the output will be a 3x20 matrix where
    output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)
    output[1, :] = params[0, :] * 1.0
    output[2, :] = params[1, :] * 3.0</p>
<p>Raises:
  TypeError: If sp_ids is not a SparseTensor, or if sp_weights is neither
    None nor SparseTensor.
  ValueError: If combiner is not one of {"mean", "sqrtn", "sum"}.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.fixed_unigram_candidate_sampler_layer">
    <p>def <span class="ident">fixed_unigram_candidate_sampler_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.fixed_unigram_candidate_sampler</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.fixed_unigram_candidate_sampler</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.fixed_unigram_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.fixed_unigram_candidate_sampler</code></strong></p>
<div class="codehilite"><pre><span></span>    def fixed_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, vocab_file=&quot;&quot;, distortion=1.0, num_reserved_ids=0, num_shards=1, shard=0, unigrams=(), seed=None, name=None)
</pre></div>


<p>Samples a set of classes using the provided (fixed) base distribution.</p>
<p>This operation randomly samples a tensor of sampled classes
(<code>sampled_candidates</code>) from the range of integers <code>[0, range_max]</code>.</p>
<p>The elements of <code>sampled_candidates</code> are drawn without replacement
(if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from
the base distribution.</p>
<p>The base distribution is read from a file or passed in as an
in-memory array. There is also an option to skew the distribution by
applying a distortion power to the weights.</p>
<p>In addition, this operation returns tensors <code>true_expected_count</code>
and <code>sampled_expected_count</code> representing the number of times each
of the target classes (<code>true_classes</code>) and the sampled
classes (<code>sampled_candidates</code>) is expected to occur in an average
tensor of sampled classes.  These values correspond to <code>Q(y|x)</code>
defined in <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">this
document</a>.
If <code>unique=True</code>, then these are post-rejection probabilities and we
compute them approximately.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  unique: A <code>bool</code>. Determines whether all sampled classes in a batch are
    unique.
  range_max: An <code>int</code>. The number of possible classes.
  vocab_file: Each valid line in this file (which should have a CSV-like
    format) corresponds to a valid word ID. IDs are in sequential order,
    starting from num_reserved_ids. The last entry in each line is expected
    to be a value corresponding to the count or relative probability. Exactly
    one of <code>vocab_file</code> and <code>unigrams</code> needs to be passed to this operation.
  distortion: The distortion is used to skew the unigram probability
    distribution.  Each weight is first raised to the distortion's power
    before adding to the internal unigram distribution. As a result,
    <code>distortion = 1.0</code> gives regular unigram sampling (as defined by the vocab
    file), and <code>distortion = 0.0</code> gives a uniform distribution.
  num_reserved_ids: Optionally some reserved IDs can be added in the range
    <code>[0, num_reserved_ids]</code> by the users. One use case is that a special
    unknown word token is used as ID 0. These IDs will have a sampling
    probability of 0.
  num_shards: A sampler can be used to sample from a subset of the original
    range in order to speed up the whole computation through parallelism. This
    parameter (together with <code>shard</code>) indicates the number of partitions that
    are being used in the overall computation.
  shard: A sampler can be used to sample from a subset of the original range
    in order to speed up the whole computation through parallelism. This
    parameter (together with <code>num_shards</code>) indicates the particular partition
    number of the operation, when partitioning is being used.
  unigrams: A list of unigram counts or probabilities, one per ID in
    sequential order. Exactly one of <code>vocab_file</code> and <code>unigrams</code> should be
    passed to this operation.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled classes.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.in_top_k_layer">
    <p>def <span class="ident">in_top_k_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.in_top_k</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.in_top_k</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.in_top_k can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.in_top_k</code></strong></p>
<div class="codehilite"><pre><span></span>    def in_top_k(predictions, targets, k, name=None)
</pre></div>


<p>Says whether the targets are in the top <code>K</code> predictions.</p>
<p>This outputs a <code>batch_size</code> bool array, an entry <code>out[i]</code> is <code>true</code> if the
prediction for the target class is among the top <code>k</code> predictions among
all predictions for example <code>i</code>. Note that the behavior of <code>InTopK</code> differs
from the <code>TopK</code> op in its handling of ties; if multiple classes have the
same prediction value and straddle the top-<code>k</code> boundary, all of those
classes are considered to be in the top <code>k</code>.</p>
<p>More formally, let</p>
<p>(predictions_i) be the predictions for all classes for example <code>i</code>,
  (targets_i) be the target class for example <code>i</code>,
  (out_i) be the output for example <code>i</code>,</p>
<p>$$out_i = predictions_{i, targets_i} \in TopKIncludingTies(predictions_i)$$</p>
<p>Args:
  predictions: A <code>Tensor</code> of type <code>float32</code>.
    A <code>batch_size</code> x <code>classes</code> tensor.
  targets: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>.
    A <code>batch_size</code> vector of class ids.
  k: An <code>int</code>. Number of top elements to look at for computing precision.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of type <code>bool</code>. Computed Precision at <code>k</code> as a <code>bool Tensor</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.l2_loss_layer">
    <p>def <span class="ident">l2_loss_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.l2_loss</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.l2_loss</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.l2_loss can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.l2_loss</code></strong></p>
<div class="codehilite"><pre><span></span>    def l2_loss(t, name=None)
</pre></div>


<p>L2 Loss.</p>
<p>Computes half the L2 norm of a tensor without the <code>sqrt</code>:</p>
<div class="codehilite"><pre><span></span>output = sum(t ** 2) / 2
</pre></div>


<p>Args:
  t: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.
    Typically 2-D, but may have any dimensions.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>t</code>. 0-D.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.l2_normalize_layer">
    <p>def <span class="ident">l2_normalize_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.l2_normalize</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.l2_normalize</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.l2_normalize can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.l2_normalize</code></strong></p>
<div class="codehilite"><pre><span></span>    def l2_normalize(x, dim, epsilon=1e-12, name=None)
</pre></div>


<p>Normalizes along dimension <code>dim</code> using an L2 norm.</p>
<p>For a 1-D tensor with <code>dim = 0</code>, computes</p>
<div class="codehilite"><pre><span></span>output = x / sqrt(max(sum(x**2), epsilon))
</pre></div>


<p>For <code>x</code> with more dimensions, independently normalizes each 1-D slice along
dimension <code>dim</code>.</p>
<p>Args:
  x: A <code>Tensor</code>.
  dim: Dimension along which to normalize.
  epsilon: A lower bound value for the norm. Will use <code>sqrt(epsilon)</code> as the
    divisor if <code>norm &lt; sqrt(epsilon)</code>.
  name: A name for this operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> with the same shape as <code>x</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.learned_unigram_candidate_sampler_layer">
    <p>def <span class="ident">learned_unigram_candidate_sampler_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.learned_unigram_candidate_sampler</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.learned_unigram_candidate_sampler</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.learned_unigram_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.learned_unigram_candidate_sampler</code></strong></p>
<div class="codehilite"><pre><span></span>    def learned_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)
</pre></div>


<p>Samples a set of classes from a distribution learned during training.</p>
<p>This operation randomly samples a tensor of sampled classes
(<code>sampled_candidates</code>) from the range of integers <code>[0, range_max]</code>.</p>
<p>The elements of <code>sampled_candidates</code> are drawn without replacement
(if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from
the base distribution.</p>
<p>The base distribution for this operation is constructed on the fly
during training.  It is a unigram distribution over the target
classes seen so far during training.  Every integer in <code>[0, range_max]</code>
begins with a weight of 1, and is incremented by 1 each time it is
seen as a target class.  The base distribution is not saved to checkpoints,
so it is reset when the model is reloaded.</p>
<p>In addition, this operation returns tensors <code>true_expected_count</code>
and <code>sampled_expected_count</code> representing the number of times each
of the target classes (<code>true_classes</code>) and the sampled
classes (<code>sampled_candidates</code>) is expected to occur in an average
tensor of sampled classes.  These values correspond to <code>Q(y|x)</code>
defined in <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">this
document</a>.
If <code>unique=True</code>, then these are post-rejection probabilities and we
compute them approximately.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  unique: A <code>bool</code>. Determines whether all sampled classes in a batch are
    unique.
  range_max: An <code>int</code>. The number of possible classes.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled classes.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.local_response_normalization_layer">
    <p>def <span class="ident">local_response_normalization_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.local_response_normalization</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.local_response_normalization</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.local_response_normalization can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.local_response_normalization</code></strong></p>
<div class="codehilite"><pre><span></span>    def lrn(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None)
</pre></div>


<p>Local Response Normalization.</p>
<p>The 4-D <code>input</code> tensor is treated as a 3-D array of 1-D vectors (along the last
dimension), and each vector is normalized independently.  Within a given vector,
each component is divided by the weighted, squared sum of inputs within
<code>depth_radius</code>.  In detail,</p>
<div class="codehilite"><pre><span></span>sqr_sum[a, b, c, d] =
    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
</pre></div>


<p>For details, see [Krizhevsky et al., ImageNet classification with deep
convolutional neural networks (NIPS 2012)]
(http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).</p>
<p>Args:
  input: A <code>Tensor</code> of type <code>float32</code>. 4-D.
  depth_radius: An optional <code>int</code>. Defaults to <code>5</code>.
    0-D.  Half-width of the 1-D normalization window.
  bias: An optional <code>float</code>. Defaults to <code>1</code>.
    An offset (usually positive to avoid dividing by 0).
  alpha: An optional <code>float</code>. Defaults to <code>1</code>.
    A scale factor, usually positive.
  beta: An optional <code>float</code>. Defaults to <code>0.5</code>. An exponent.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of type <code>float32</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.log_softmax_layer">
    <p>def <span class="ident">log_softmax_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.log_softmax</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.log_softmax</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.log_softmax can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.log_softmax</code></strong></p>
<div class="codehilite"><pre><span></span>    def log_softmax(logits, name=None)
</pre></div>


<p>Computes log softmax activations.</p>
<p>For each batch <code>i</code> and class <code>j</code> we have</p>
<div class="codehilite"><pre><span></span>logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))
</pre></div>


<p>Args:
  logits: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    2-D with shape <code>[batch_size, num_classes]</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>logits</code>. Same shape as <code>logits</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.log_uniform_candidate_sampler_layer">
    <p>def <span class="ident">log_uniform_candidate_sampler_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.log_uniform_candidate_sampler</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.log_uniform_candidate_sampler</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.log_uniform_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.log_uniform_candidate_sampler</code></strong></p>
<div class="codehilite"><pre><span></span>    def log_uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)
</pre></div>


<p>Samples a set of classes using a log-uniform (Zipfian) base distribution.</p>
<p>This operation randomly samples a tensor of sampled classes
(<code>sampled_candidates</code>) from the range of integers <code>[0, range_max]</code>.</p>
<p>The elements of <code>sampled_candidates</code> are drawn without replacement
(if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from
the base distribution.</p>
<p>The base distribution for this operation is an approximately log-uniform
or Zipfian distribution:</p>
<p><code>P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)</code></p>
<p>This sampler is useful when the target classes approximately follow such
a distribution - for example, if the classes represent words in a lexicon
sorted in decreasing order of frequency. If your classes are not ordered by
decreasing frequency, do not use this op.</p>
<p>In addition, this operation returns tensors <code>true_expected_count</code>
and <code>sampled_expected_count</code> representing the number of times each
of the target classes (<code>true_classes</code>) and the sampled
classes (<code>sampled_candidates</code>) is expected to occur in an average
tensor of sampled classes.  These values correspond to <code>Q(y|x)</code>
defined in <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">this
document</a>.
If <code>unique=True</code>, then these are post-rejection probabilities and we
compute them approximately.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  unique: A <code>bool</code>. Determines whether all sampled classes in a batch are
    unique.
  range_max: An <code>int</code>. The number of possible classes.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled classes.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.lrn_layer">
    <p>def <span class="ident">lrn_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.lrn</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.lrn</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.lrn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.lrn</code></strong></p>
<div class="codehilite"><pre><span></span>    def lrn(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None)
</pre></div>


<p>Local Response Normalization.</p>
<p>The 4-D <code>input</code> tensor is treated as a 3-D array of 1-D vectors (along the last
dimension), and each vector is normalized independently.  Within a given vector,
each component is divided by the weighted, squared sum of inputs within
<code>depth_radius</code>.  In detail,</p>
<div class="codehilite"><pre><span></span>sqr_sum[a, b, c, d] =
    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
</pre></div>


<p>For details, see [Krizhevsky et al., ImageNet classification with deep
convolutional neural networks (NIPS 2012)]
(http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).</p>
<p>Args:
  input: A <code>Tensor</code> of type <code>float32</code>. 4-D.
  depth_radius: An optional <code>int</code>. Defaults to <code>5</code>.
    0-D.  Half-width of the 1-D normalization window.
  bias: An optional <code>float</code>. Defaults to <code>1</code>.
    An offset (usually positive to avoid dividing by 0).
  alpha: An optional <code>float</code>. Defaults to <code>1</code>.
    A scale factor, usually positive.
  beta: An optional <code>float</code>. Defaults to <code>0.5</code>. An exponent.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of type <code>float32</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.make_all_layer">
    <p>def <span class="ident">make_all_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.make_all</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.make_all</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.make_all can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.make_all</code></strong></p>
<div class="codehilite"><pre><span></span>    def make_all(module_name, doc_string_modules=None)
</pre></div>


<p>Generate <code>__all__</code> from the docstring of one or more modules.</p>
<p>Usage: <code>make_all(__name__)</code> or
<code>make_all(__name__, [sys.modules(__name__), other_module])</code>. The doc string
modules must each a docstring, and <code>__all__</code> will contain all symbols with
<code>@@</code> references, where that symbol currently exists in the module named
<code>module_name</code>.</p>
<p>Args:
  module_name: The name of the module (usually <code>__name__</code>).
  doc_string_modules: a list of modules from which to take docstring.
  If None, then a list containing only the module named <code>module_name</code> is used.</p>
<p>Returns:
  A list suitable for use as <code>__all__</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map__compute_sampled_logits">
    <p>def <span class="ident">map__compute_sampled_logits</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn._compute_sampled_logits</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn._compute_sampled_logits</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn._compute_sampled_logits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn._compute_sampled_logits</code> </strong></p>
<div class="codehilite"><pre><span></span>    def _compute_sampled_logits(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, subtract_log_q=True, remove_accidental_hits=False, partition_strategy=&quot;mod&quot;, name=None)
</pre></div>


<p>Helper function for nce_loss and sampled_softmax_loss functions.</p>
<p>Computes sampled output training logits and labels suitable for implementing
e.g. noise-contrastive estimation (see nce_loss) or sampled softmax (see
sampled_softmax_loss).</p>
<p>Note: In the case where num_true &gt; 1, we assign to each target class
the target probability 1 / num_true so that the target probabilities
sum to 1 per-example.</p>
<p>Args:
  weights: A <code>Tensor</code> of shape <code>[num_classes, dim]</code>, or a list of <code>Tensor</code>
      objects whose concatenation along dimension 0 has shape
      <code>[num_classes, dim]</code>.  The (possibly-partitioned) class embeddings.
  biases: A <code>Tensor</code> of shape <code>[num_classes]</code>.  The class biases.
  inputs: A <code>Tensor</code> of shape <code>[batch_size, dim]</code>.  The forward
      activations of the input network.
  labels: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
      num_true]</code>. The target classes.  Note that this format differs from
      the <code>labels</code> argument of <code>nn.softmax_cross_entropy_with_logits</code>.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  num_classes: An <code>int</code>. The number of possible classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  sampled_values: a tuple of (<code>sampled_candidates</code>, <code>true_expected_count</code>,
      <code>sampled_expected_count</code>) returned by a <code>*_candidate_sampler</code> function.
      (if None, we default to <code>log_uniform_candidate_sampler</code>)
  subtract_log_q: A <code>bool</code>.  whether to subtract the log expected count of
      the labels in the sample to get the logits of the true labels.
      Default is True.  Turn off for Negative Sampling.
  remove_accidental_hits:  A <code>bool</code>.  whether to remove "accidental hits"
      where a sampled class equals one of the target classes.  Default is
      False.
  partition_strategy: A string specifying the partitioning strategy, relevant
      if <code>len(weights) &gt; 1</code>. Currently <code>"div"</code> and <code>"mod"</code> are supported.
      Default is <code>"mod"</code>. See <code>tf.nn.embedding_lookup</code> for more details.
  name: A name for the operation (optional).
Returns:
  out_logits, out_labels: <code>Tensor</code> objects each with shape
      <code>[batch_size, num_true + num_sampled]</code>, for passing to either
      <code>nn.sigmoid_cross_entropy_with_logits</code> (NCE) or
      <code>nn.softmax_cross_entropy_with_logits</code> (sampled softmax).</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map__sum_rows">
    <p>def <span class="ident">map__sum_rows</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn._sum_rows</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn._sum_rows</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn._sum_rows can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn._sum_rows</code> </strong></p>
<div class="codehilite"><pre><span></span>    def _sum_rows(x)
</pre></div>


<p>Returns a vector summing up each row of the matrix x.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_all_candidate_sampler">
    <p>def <span class="ident">map_all_candidate_sampler</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.all_candidate_sampler</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.all_candidate_sampler</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.all_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.all_candidate_sampler</code> </strong></p>
<div class="codehilite"><pre><span></span>    def all_candidate_sampler(true_classes, num_true, num_sampled, unique, seed=None, name=None)
</pre></div>


<p>Generate the set of all classes.</p>
<p>Deterministically generates and returns the set of all possible classes.
For testing purposes.  There is no need to use this, since you might as
well use full softmax or full logistic regression.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of possible classes.
  unique: A <code>bool</code>. Ignored.
    unique.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    This operation deterministically returns the entire range
    <code>[0, num_sampled]</code>.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>. All returned values are 1.0.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>. All returned values are 1.0.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_avg_pool">
    <p>def <span class="ident">map_avg_pool</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.avg_pool</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.avg_pool</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.avg_pool can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.avg_pool</code> </strong></p>
<div class="codehilite"><pre><span></span>    def avg_pool(value, ksize, strides, padding, data_format=&quot;NHWC&quot;, name=None)
</pre></div>


<p>Performs the average pooling on the input.</p>
<p>Each entry in <code>output</code> is the mean of the corresponding size <code>ksize</code>
window in <code>value</code>.</p>
<p>Args:
  value: A 4-D <code>Tensor</code> of shape <code>[batch, height, width, channels]</code> and type
    <code>float32</code>, <code>float64</code>, <code>qint8</code>, <code>quint8</code>, or <code>qint32</code>.
  ksize: A list of ints that has length &gt;= 4.
    The size of the window for each dimension of the input tensor.
  strides: A list of ints that has length &gt;= 4.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm.
  data_format: A string. 'NHWC' and 'NCHW' are supported.
  name: Optional name for the operation.</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>value</code>.  The average pooled output tensor.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_batch_norm_with_global_normalization">
    <p>def <span class="ident">map_batch_norm_with_global_normalization</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.batch_norm_with_global_normalization</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.batch_norm_with_global_normalization</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.batch_norm_with_global_normalization can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.batch_norm_with_global_normalization</code> </strong></p>
<div class="codehilite"><pre><span></span>    def batch_norm_with_global_normalization(t, m, v, beta, gamma, variance_epsilon, scale_after_normalization, name=None)
</pre></div>


<p>Batch normalization.</p>
<p>This op is deprecated. See <code>tf.nn.batch_normalization</code>.</p>
<p>Args:
  t: A 4D input Tensor.
  m: A 1D mean Tensor with size matching the last dimension of t.
    This is the first output from tf.nn.moments,
    or a saved moving average thereof.
  v: A 1D variance Tensor with size matching the last dimension of t.
    This is the second output from tf.nn.moments,
    or a saved moving average thereof.
  beta: A 1D beta Tensor with size matching the last dimension of t.
    An offset to be added to the normalized tensor.
  gamma: A 1D gamma Tensor with size matching the last dimension of t.
    If "scale_after_normalization" is true, this tensor will be multiplied
    with the normalized tensor.
  variance_epsilon: A small float number to avoid dividing by 0.
  scale_after_normalization: A bool indicating whether the resulted tensor
    needs to be multiplied with gamma.
  name: A name for this operation (optional).</p>
<p>Returns:
   A batch-normalized <code>t</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_batch_normalization">
    <p>def <span class="ident">map_batch_normalization</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.batch_normalization</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.batch_normalization</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.batch_normalization can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.batch_normalization</code> </strong></p>
<div class="codehilite"><pre><span></span>    def batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None)
</pre></div>


<p>Batch normalization.</p>
<p>As described in http://arxiv.org/abs/1502.03167.
Normalizes a tensor by <code>mean</code> and <code>variance</code>, and applies (optionally) a
<code>scale</code> (\gamma) to it, as well as an <code>offest</code> (eta):</p>
<p>(rac{\gamma(x-\mu)}{\sigma}+eta)</p>
<p><code>mean</code>, <code>variance</code>, <code>offset</code> and <code>scale</code> are all expected to be of one of two
shapes:
  * In all generality, they can have the same number of dimensions as the
    input <code>x</code>, with identical sizes as <code>x</code> for the dimensions that are not
    normalized over (the 'depth' dimension(s)), and dimension 1 for the
    others which are being normalized over.
    <code>mean</code> and <code>variance</code> in this case would typically be the outputs of
    <code>tf.nn.moments(..., keep_dims=True)</code> during training, or running averages
    thereof during inference.
  * In the common case where the 'depth' dimension is the last dimension in
    the input tensor <code>x</code>, they may be one dimensional tensors of the same
    size as the 'depth' dimension.
    This is the case for example for the common <code>[batch, depth]</code> layout of
    fully-connected layers, and <code>[batch, height, width, depth]</code> for
    convolutions.
    <code>mean</code> and <code>variance</code> in this case would typically be the outputs of
    <code>tf.nn.moments(..., keep_dims=False)</code> during training, or running averages
    thereof during inference.</p>
<p>Args:
  x: Input <code>Tensor</code> of arbitrary dimensionality.
  mean: A mean <code>Tensor</code>.
  variance: A variance <code>Tensor</code>.
  offset: An offset <code>Tensor</code>, often denoted (eta) in equations, or
    None. If present, will be added to the normalized tensor.
  scale: A scale <code>Tensor</code>, often denoted (\gamma) in equations, or
    <code>None</code>. If present, the scale is applied to the normalized tensor.
  variance_epsilon: A small float number to avoid dividing by 0.
  name: A name for this operation (optional).</p>
<p>Returns:
  the normalized, scaled, offset tensor.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_bias_add">
    <p>def <span class="ident">map_bias_add</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.bias_add</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.bias_add</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.bias_add can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.bias_add</code> </strong></p>
<div class="codehilite"><pre><span></span>    def bias_add(value, bias, data_format=None, name=None)
</pre></div>


<p>Adds <code>bias</code> to <code>value</code>.</p>
<p>This is (mostly) a special case of <code>tf.add</code> where <code>bias</code> is restricted to 1-D.
Broadcasting is supported, so <code>value</code> may have any number of dimensions.
Unlike <code>tf.add</code>, the type of <code>bias</code> is allowed to differ from <code>value</code> in the
case where both types are quantized.</p>
<p>Args:
  value: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>,
    <code>int16</code>, <code>int8</code>, or <code>complex64</code>.
  bias: A 1-D <code>Tensor</code> with size matching the last dimension of <code>value</code>.
    Must be the same type as <code>value</code> unless <code>value</code> is a quantized type,
    in which case a different quantized type may be used.
  data_format: A string. 'NHWC' and 'NCHW' are supported.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>value</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_bias_add_grad">
    <p>def <span class="ident">map_bias_add_grad</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.bias_add_grad</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.bias_add_grad</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.bias_add_grad can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.bias_add_grad</code> </strong></p>
<div class="codehilite"><pre><span></span>    def bias_add_grad(out_backprop, data_format=None, name=None)
</pre></div>


<p>The backward operation for "BiasAdd" on the "bias" tensor.</p>
<p>It accumulates all the values from out_backprop into the feature dimension.
For NHWC data format, the feature dimension is the last. For NCHW data format,
the feature dimension is the third-to-last.</p>
<p>Args:
  out_backprop: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.
    Any number of dimensions.
  data_format: An optional <code>string</code> from: <code>"NHWC", "NCHW"</code>. Defaults to <code>"NHWC"</code>.
    Specify the data format of the input and output data. With the
    default format "NHWC", the bias tensor will be added to the last dimension
    of the value tensor.
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
    The tensor will be added to "in_channels", the third-to-the-last
        dimension.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>out_backprop</code>.
  1-D with size the feature dimension of <code>out_backprop</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_bias_add_v1">
    <p>def <span class="ident">map_bias_add_v1</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.bias_add_v1</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.bias_add_v1</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.bias_add_v1 can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.bias_add_v1</code> </strong></p>
<div class="codehilite"><pre><span></span>    def bias_add_v1(value, bias, name=None)
</pre></div>


<p>Adds <code>bias</code> to <code>value</code>.</p>
<p>This is a deprecated version of bias_add and will soon to be removed.</p>
<p>This is (mostly) a special case of <code>tf.add</code> where <code>bias</code> is restricted to 1-D.
Broadcasting is supported, so <code>value</code> may have any number of dimensions.
Unlike <code>tf.add</code>, the type of <code>bias</code> is allowed to differ from <code>value</code> in the
case where both types are quantized.</p>
<p>Args:
  value: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>,
    <code>int16</code>, <code>int8</code>, or <code>complex64</code>.
  bias: A 1-D <code>Tensor</code> with size matching the last dimension of <code>value</code>.
    Must be the same type as <code>value</code> unless <code>value</code> is a quantized type,
    in which case a different quantized type may be used.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>value</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_bidirectional_rnn">
    <p>def <span class="ident">map_bidirectional_rnn</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.bidirectional_rnn</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.bidirectional_rnn</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.bidirectional_rnn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.bidirectional_rnn</code> </strong></p>
<div class="codehilite"><pre><span></span>    def bidirectional_rnn(cell_fw, cell_bw, inputs, initial_state_fw=None, initial_state_bw=None, dtype=None, sequence_length=None, scope=None)
</pre></div>


<p>Creates a bidirectional recurrent neural network.</p>
<p>Similar to the unidirectional case above (rnn) but takes input and builds
independent forward and backward RNNs with the final forward and backward
outputs depth-concatenated, such that the output will have the format
[time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of
forward and backward cell must match. The initial state for both directions
is zero by default (but can be set optionally) and no intermediate states are
ever returned -- the network is fully unrolled for the given (passed in)
length(s) of the sequence(s) or completely unrolled if length(s) is not given.</p>
<p>Args:
  cell_fw: An instance of RNNCell, to be used for forward direction.
  cell_bw: An instance of RNNCell, to be used for backward direction.
  inputs: A length T list of inputs, each a tensor of shape
    [batch_size, cell.input_size].
  initial_state_fw: (optional) An initial state for the forward RNN.
    This must be a tensor of appropriate type and shape
    [batch_size x cell.state_size].
  initial_state_bw: (optional) Same as for initial_state_fw.
  dtype: (optional) The data type for the initial state.  Required if either
    of the initial states are not provided.
  sequence_length: (optional) An int32/int64 vector, size [batch_size],
    containing the actual lengths for each of the sequences.
  scope: VariableScope for the created subgraph; defaults to "BiRNN"</p>
<p>Returns:
  A tuple (outputs, output_state_fw, output_state_bw) where:
    outputs is a length T list of outputs (one for each input), which
    are depth-concatenated forward and backward outputs
    output_state_fw is the final state of the forward rnn
    output_state_bw is the final state of the backward rnn</p>
<p>Raises:
  TypeError: If "cell_fw" or "cell_bw" is not an instance of RNNCell.
  ValueError: If inputs is None or an empty list.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_compute_accidental_hits">
    <p>def <span class="ident">map_compute_accidental_hits</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.compute_accidental_hits</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.compute_accidental_hits</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.compute_accidental_hits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.compute_accidental_hits</code> </strong></p>
<div class="codehilite"><pre><span></span>    def compute_accidental_hits(true_classes, sampled_candidates, num_true, seed=None, name=None)
</pre></div>


<p>Compute the position ids in <code>sampled_candidates</code> matching <code>true_classes</code>.</p>
<p>In Candidate Sampling, this operation facilitates virtually removing
sampled classes which happen to match target classes.  This is done
in Sampled Softmax and Sampled Logistic.</p>
<p>See our <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">Candidate Sampling Algorithms
Reference</a>.</p>
<p>We presuppose that the <code>sampled_candidates</code> are unique.</p>
<p>We call it an 'accidental hit' when one of the target classes
matches one of the sampled classes.  This operation reports
accidental hits as triples <code>(index, id, weight)</code>, where <code>index</code>
represents the row number in <code>true_classes</code>, <code>id</code> represents the
position in <code>sampled_candidates</code>, and weight is <code>-FLOAT_MAX</code>.</p>
<p>The result of this op should be passed through a <code>sparse_to_dense</code>
operation, then added to the logits of the sampled classes. This
removes the contradictory effect of accidentally sampling the true
target classes as noise classes for the same example.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled_candidates output of CandidateSampler.
  num_true: An <code>int</code>.  The number of target classes per training example.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  indices: A <code>Tensor</code> of type <code>int32</code> and shape <code>[num_accidental_hits]</code>.
    Values indicate rows in <code>true_classes</code>.
  ids: A <code>Tensor</code> of type <code>int64</code> and shape <code>[num_accidental_hits]</code>.
    Values indicate positions in <code>sampled_candidates</code>.
  weights: A <code>Tensor</code> of type <code>float</code> and shape <code>[num_accidental_hits]</code>.
    Each value is <code>-FLOAT_MAX</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_conv2d">
    <p>def <span class="ident">map_conv2d</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.conv2d</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.conv2d</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.conv2d can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.conv2d</code> </strong></p>
<div class="codehilite"><pre><span></span>    def conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)
</pre></div>


<p>Computes a 2-D convolution given 4-D <code>input</code> and <code>filter</code> tensors.</p>
<p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code>
and a filter / kernel tensor of shape
<code>[filter_height, filter_width, in_channels, out_channels]</code>, this op
performs the following:</p>
<ol>
<li>Flattens the filter to a 2-D matrix with shape
   <code>[filter_height * filter_width * in_channels, output_channels]</code>.</li>
<li>Extracts image patches from the input tensor to form a <em>virtual</em>
   tensor of shape <code>[batch, out_height, out_width,
   filter_height * filter_width * in_channels]</code>.</li>
<li>For each patch, right-multiplies the filter matrix and the image patch
   vector.</li>
</ol>
<p>In detail, with the default NHWC format,</p>
<div class="codehilite"><pre><span></span>output[b, i, j, k] =
    sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *
                    filter[di, dj, q, k]
</pre></div>


<p>Must have <code>strides[0] = strides[3] = 1</code>.  For the most common case of the same
horizontal and vertices strides, <code>strides = [1, stride, stride, 1]</code>.</p>
<p>Args:
  input: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
  filter: A <code>Tensor</code>. Must have the same type as <code>input</code>.
  strides: A list of <code>ints</code>.
    1-D of length 4.  The stride of the sliding window for each dimension
    of <code>input</code>. Must be in the same order as the dimension specified with format.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional <code>bool</code>. Defaults to <code>True</code>.
  data_format: An optional <code>string</code> from: <code>"NHWC", "NCHW"</code>. Defaults to <code>"NHWC"</code>.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>input</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_conv2d_backprop_filter">
    <p>def <span class="ident">map_conv2d_backprop_filter</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.conv2d_backprop_filter</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.conv2d_backprop_filter</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.conv2d_backprop_filter can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.conv2d_backprop_filter</code> </strong></p>
<div class="codehilite"><pre><span></span>    def conv2d_backprop_filter(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)
</pre></div>


<p>Computes the gradients of convolution with respect to the filter.</p>
<p>Args:
  input: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.
  filter_sizes: A <code>Tensor</code> of type <code>int32</code>.
    An integer vector representing the tensor shape of <code>filter</code>,
    where <code>filter</code> is a 4-D
    <code>[filter_height, filter_width, in_channels, out_channels]</code> tensor.
  out_backprop: A <code>Tensor</code>. Must have the same type as <code>input</code>.
    4-D with shape <code>[batch, out_height, out_width, out_channels]</code>.
    Gradients w.r.t. the output of the convolution.
  strides: A list of <code>ints</code>.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional <code>bool</code>. Defaults to <code>True</code>.
  data_format: An optional <code>string</code> from: <code>"NHWC", "NCHW"</code>. Defaults to <code>"NHWC"</code>.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>input</code>. 4-D with shape
  <code>[filter_height, filter_width, in_channels, out_channels]</code>.  Gradient w.r.t.
  the <code>filter</code> input of the convolution.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_conv2d_backprop_input">
    <p>def <span class="ident">map_conv2d_backprop_input</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.conv2d_backprop_input</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.conv2d_backprop_input</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.conv2d_backprop_input can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.conv2d_backprop_input</code> </strong></p>
<div class="codehilite"><pre><span></span>    def conv2d_backprop_input(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)
</pre></div>


<p>Computes the gradients of convolution with respect to the input.</p>
<p>Args:
  input_sizes: A <code>Tensor</code> of type <code>int32</code>.
    An integer vector representing the shape of <code>input</code>,
    where <code>input</code> is a 4-D <code>[batch, height, width, channels]</code> tensor.
  filter: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    4-D with shape
    <code>[filter_height, filter_width, in_channels, out_channels]</code>.
  out_backprop: A <code>Tensor</code>. Must have the same type as <code>filter</code>.
    4-D with shape <code>[batch, out_height, out_width, out_channels]</code>.
    Gradients w.r.t. the output of the convolution.
  strides: A list of <code>ints</code>.
    The stride of the sliding window for each dimension of the input
    of the convolution. Must be in the same order as the dimension specified with
    format.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  use_cudnn_on_gpu: An optional <code>bool</code>. Defaults to <code>True</code>.
  data_format: An optional <code>string</code> from: <code>"NHWC", "NCHW"</code>. Defaults to <code>"NHWC"</code>.
    Specify the data format of the input and output data. With the
    default format "NHWC", the data is stored in the order of:
        [batch, in_height, in_width, in_channels].
    Alternatively, the format could be "NCHW", the data storage order of:
        [batch, in_channels, in_height, in_width].
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>filter</code>.
  4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.  Gradient
  w.r.t. the input of the convolution.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_conv2d_transpose">
    <p>def <span class="ident">map_conv2d_transpose</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.conv2d_transpose</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.conv2d_transpose</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.conv2d_transpose can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.conv2d_transpose</code> </strong></p>
<div class="codehilite"><pre><span></span>    def conv2d_transpose(value, filter, output_shape, strides, padding=&quot;SAME&quot;, name=None)
</pre></div>


<p>The transpose of <code>conv2d</code>.</p>
<p>This operation is sometimes called "deconvolution" after (Deconvolutional
Networks)[http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf], but is
actually the transpose (gradient) of <code>conv2d</code> rather than an actual
deconvolution.</p>
<p>Args:
  value: A 4-D <code>Tensor</code> of type <code>float</code> and shape
    <code>[batch, height, width, in_channels]</code>.
  filter: A 4-D <code>Tensor</code> with the same type as <code>value</code> and shape
    <code>[height, width, output_channels, in_channels]</code>.  <code>filter</code>'s
    <code>in_channels</code> dimension must match that of <code>value</code>.
  output_shape: A 1-D <code>Tensor</code> representing the output shape of the
    deconvolution op.
  strides: A list of ints. The stride of the sliding window for each
    dimension of the input tensor.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm.
  name: Optional name for the returned tensor.</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>value</code>.</p>
<p>Raises:
  ValueError: If input/output depth does not match <code>filter</code>'s shape, or if
    padding is other than <code>'VALID'</code> or <code>'SAME'</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_depthwise_conv2d">
    <p>def <span class="ident">map_depthwise_conv2d</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.depthwise_conv2d</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.depthwise_conv2d</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.depthwise_conv2d can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.depthwise_conv2d</code> </strong></p>
<div class="codehilite"><pre><span></span>    def depthwise_conv2d(input, filter, strides, padding, name=None)
</pre></div>


<p>Depthwise 2-D convolution.</p>
<p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code>
and a filter tensor of shape
<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>
containing <code>in_channels</code> convolutional filters of depth 1, <code>depthwise_conv2d</code>
applies a different filter to each input channel (expanding from 1 channel
to <code>channel_multiplier</code> channels for each), then concatenates the results
together.  The output has <code>in_channels * channel_multiplier</code> channels.</p>
<p>In detail,</p>
<div class="codehilite"><pre><span></span>output[b, i, j, k * channel_multiplier + q] =
    sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *
                 filter[di, dj, k, q]
</pre></div>


<p>Must have <code>strides[0] = strides[3] = 1</code>.  For the most common case of the
same horizontal and vertical strides, <code>strides = [1, stride, stride, 1]</code>.</p>
<p>Args:
  input: 4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.
  filter: 4-D with shape
    <code>[filter_height, filter_width, in_channels, channel_multiplier]</code>.
  strides: 1-D of size 4.  The stride of the sliding window for each
    dimension of <code>input</code>.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>.  The padding algorithm.
  name: A name for this operation (optional).</p>
<p>Returns:
  A 4-D <code>Tensor</code> of shape
  <code>[batch, out_height, out_width, in_channels * channel_multiplier].</code></p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_depthwise_conv2d_native">
    <p>def <span class="ident">map_depthwise_conv2d_native</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.depthwise_conv2d_native</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.depthwise_conv2d_native</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.depthwise_conv2d_native can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.depthwise_conv2d_native</code> </strong></p>
<div class="codehilite"><pre><span></span>    def depthwise_conv2d_native(input, filter, strides, padding, name=None)
</pre></div>


<p>Computes a 2-D depthwise convolution given 4-D <code>input</code> and <code>filter</code> tensors.</p>
<p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code>
and a filter / kernel tensor of shape
<code>[filter_height, filter_width, in_channels, channel_multiplier]</code>, containing
<code>in_channels</code> convolutional filters of depth 1, <code>depthwise_conv2d</code> applies
a different filter to each input channel (expanding from 1 channel to
<code>channel_multiplier</code> channels for each), then concatenates the results
together. Thus, the output has <code>in_channels * channel_multiplier</code> channels.</p>
<p>for k in 0..in_channels-1
  for q in 0..channel_multiplier-1
    output[b, i, j, k * channel_multiplier + q] =
      sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *
                        filter[di, dj, k, q]</p>
<p>Must have <code>strides[0] = strides[3] = 1</code>.  For the most common case of the same
horizontal and vertices strides, <code>strides = [1, stride, stride, 1]</code>.</p>
<p>Args:
  input: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
  filter: A <code>Tensor</code>. Must have the same type as <code>input</code>.
  strides: A list of <code>ints</code>.
    1-D of length 4.  The stride of the sliding window for each dimension
    of <code>input</code>.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>input</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_depthwise_conv2d_native_backprop_filter">
    <p>def <span class="ident">map_depthwise_conv2d_native_backprop_filter</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.depthwise_conv2d_native_backprop_filter</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.depthwise_conv2d_native_backprop_filter</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.depthwise_conv2d_native_backprop_filter can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.depthwise_conv2d_native_backprop_filter</code> </strong></p>
<div class="codehilite"><pre><span></span>    def depthwise_conv2d_native_backprop_filter(input, filter_sizes, out_backprop, strides, padding, name=None)
</pre></div>


<p>Computes the gradients of depthwise convolution with respect to the filter.</p>
<p>Args:
  input: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.
  filter_sizes: A <code>Tensor</code> of type <code>int32</code>.
    An integer vector representing the tensor shape of <code>filter</code>,
    where <code>filter</code> is a 4-D
    <code>[filter_height, filter_width, in_channels, depthwise_multiplier]</code> tensor.
  out_backprop: A <code>Tensor</code>. Must have the same type as <code>input</code>.
    4-D with shape <code>[batch, out_height, out_width, out_channels]</code>.
    Gradients w.r.t. the output of the convolution.
  strides: A list of <code>ints</code>.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>input</code>. 4-D with shape
  <code>[filter_height, filter_width, in_channels, out_channels]</code>.  Gradient w.r.t.
  the <code>filter</code> input of the convolution.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_depthwise_conv2d_native_backprop_input">
    <p>def <span class="ident">map_depthwise_conv2d_native_backprop_input</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.depthwise_conv2d_native_backprop_input</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.depthwise_conv2d_native_backprop_input</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.depthwise_conv2d_native_backprop_input can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.depthwise_conv2d_native_backprop_input</code> </strong></p>
<div class="codehilite"><pre><span></span>    def depthwise_conv2d_native_backprop_input(input_sizes, filter, out_backprop, strides, padding, name=None)
</pre></div>


<p>Computes the gradients of depthwise convolution with respect to the input.</p>
<p>Args:
  input_sizes: A <code>Tensor</code> of type <code>int32</code>.
    An integer vector representing the shape of <code>input</code>,
    where <code>input</code> is a 4-D <code>[batch, height, width, channels]</code> tensor.
  filter: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    4-D with shape
    <code>[filter_height, filter_width, in_channels, depthwise_multiplier]</code>.
  out_backprop: A <code>Tensor</code>. Must have the same type as <code>filter</code>.
    4-D with shape <code>[batch, out_height, out_width, out_channels]</code>.
    Gradients w.r.t. the output of the convolution.
  strides: A list of <code>ints</code>.
    The stride of the sliding window for each dimension of the input
    of the convolution.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>filter</code>.
  4-D with shape <code>[batch, in_height, in_width, in_channels]</code>.  Gradient
  w.r.t. the input of the convolution.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_dropout">
    <p>def <span class="ident">map_dropout</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.dropout</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.dropout</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.dropout can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.dropout</code> </strong></p>
<div class="codehilite"><pre><span></span>    def dropout(x, keep_prob, noise_shape=None, seed=None, name=None)
</pre></div>


<p>Computes dropout.</p>
<p>With probability <code>keep_prob</code>, outputs the input element scaled up by
<code>1 / keep_prob</code>, otherwise outputs <code>0</code>.  The scaling is so that the expected
sum is unchanged.</p>
<p>By default, each element is kept or dropped independently.  If <code>noise_shape</code>
is specified, it must be
<a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">broadcastable</a>
to the shape of <code>x</code>, and only dimensions with <code>noise_shape[i] == shape(x)[i]</code>
will make independent decisions.  For example, if <code>shape(x) = [k, l, m, n]</code>
and <code>noise_shape = [k, 1, 1, n]</code>, each batch and channel component will be
kept independently and each row and column will be kept or not kept together.</p>
<p>Args:
  x: A tensor.
  keep_prob: A scalar <code>Tensor</code> with the same type as x. The probability
    that each element is kept.
  noise_shape: A 1-D <code>Tensor</code> of type <code>int32</code>, representing the
    shape for randomly generated keep/drop flags.
  seed: A Python integer. Used to create random seeds. See
    <a href="../../api_docs/python/constant_op.md#set_random_seed"><code>set_random_seed</code></a>
    for behavior.
  name: A name for this operation (optional).</p>
<p>Returns:
  A Tensor of the same shape of <code>x</code>.</p>
<p>Raises:
  ValueError: If <code>keep_prob</code> is not in <code>(0, 1]</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_dynamic_rnn">
    <p>def <span class="ident">map_dynamic_rnn</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.dynamic_rnn</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.dynamic_rnn</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.dynamic_rnn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.dynamic_rnn</code> </strong></p>
<div class="codehilite"><pre><span></span>    def dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None)
</pre></div>


<p>Creates a recurrent neural network specified by RNNCell "cell".</p>
<p>This function is functionally identical to the function <code>rnn</code> above, but
performs fully dynamic unrolling of <code>inputs</code>.</p>
<p>Unlike <code>rnn</code>, the input <code>inputs</code> is not a Python list of <code>Tensors</code>.  Instead,
it is a single <code>Tensor</code> where the maximum time is either the first or second
dimension (see the parameter <code>time_major</code>).  The corresponding output is
a single <code>Tensor</code> having the same number of time steps and batch size.</p>
<p>The parameter <code>sequence_length</code> is required and dynamic calculation is
automatically performed.</p>
<p>Args:
  cell: An instance of RNNCell.
  inputs: The RNN inputs.
    If time_major == False (default), this must be a tensor of shape:
      <code>[batch_size, max_time, cell.input_size]</code>.
    If time_major == True, this must be a tensor of shape:
      <code>[max_time, batch_size, cell.input_size]</code>.
  sequence_length: (optional) An int32/int64 vector sized <code>[batch_size]</code>.
  initial_state: (optional) An initial state for the RNN.  This must be
    a tensor of appropriate type and shape <code>[batch_size x cell.state_size]</code>.
  dtype: (optional) The data type for the initial state.  Required if
    initial_state is not provided.
  parallel_iterations: (Default: 32).  The number of iterations to run in
    parallel.  Those operations which do not have any temporal dependency
    and can be run in parallel, will be.  This parameter trades off
    time for space.  Values &gt;&gt; 1 use more memory but take less time,
    while smaller values use less memory but computations take longer.
  swap_memory: Swap the tensors produced in forward inference but needed
    for back prop from GPU to CPU.
  time_major: The shape format of the <code>inputs</code> and <code>outputs</code> Tensors.
    If true, these <code>Tensors</code> must be shaped <code>[max_time, batch_size, depth]</code>.
    If false, these <code>Tensors</code> must be shaped <code>[batch_size, max_time, depth]</code>.
    Using time_major = False is a bit more efficient because it avoids
    transposes at the beginning and end of the RNN calculation.  However,
    most TensorFlow data is batch-major, so by default this function
    accepts input and emits output in batch-major form.
  scope: VariableScope for the created subgraph; defaults to "RNN".</p>
<p>Returns:
  A pair (outputs, state) where:
    outputs: The RNN output <code>Tensor</code>.
      If time_major == False (default), this will be a <code>Tensor</code> shaped:
        <code>[batch_size, max_time, cell.output_size]</code>.
      If time_major == True, this will be a <code>Tensor</code> shaped:
        <code>[max_time, batch_size, cell.output_size]</code>.
    state: The final state, shaped:
      <code>[batch_size, cell.state_size]</code>.</p>
<p>Raises:
  TypeError: If "cell" is not an instance of RNNCell.
  ValueError: If inputs is None or an empty list.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_elu">
    <p>def <span class="ident">map_elu</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.elu</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.elu</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.elu can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.elu</code> </strong></p>
<div class="codehilite"><pre><span></span>    def elu(features, name=None)
</pre></div>


<p>Computes exponential linear: <code>exp(features) - 1</code> if &lt; 0, <code>features</code> otherwise.</p>
<p>See <a href="http://arxiv.org/abs/1511.07289">Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
</a></p>
<p>Args:
  features: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_embedding_lookup">
    <p>def <span class="ident">map_embedding_lookup</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.embedding_lookup</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.embedding_lookup</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.embedding_lookup can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.embedding_lookup</code> </strong></p>
<div class="codehilite"><pre><span></span>    def embedding_lookup(params, ids, partition_strategy=&quot;mod&quot;, name=None, validate_indices=True)
</pre></div>


<p>Looks up <code>ids</code> in a list of embedding tensors.</p>
<p>This function is used to perform parallel lookups on the list of
tensors in <code>params</code>.  It is a generalization of
<a href="../../api_docs/python/array_ops.md#gather"><code>tf.gather()</code></a>, where <code>params</code> is
interpreted as a partition of a larger embedding tensor.</p>
<p>If <code>len(params) &gt; 1</code>, each element <code>id</code> of <code>ids</code> is partitioned between
the elements of <code>params</code> according to the <code>partition_strategy</code>.
In all strategies, if the id space does not evenly divide the number of
partitions, each of the first <code>(max_id + 1) % len(params)</code> partitions will
be assigned one more id.</p>
<p>If <code>partition_strategy</code> is <code>"mod"</code>, we assign each id to partition
<code>p = id % len(params)</code>. For instance,
13 ids are split across 5 partitions as:
<code>[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]</code></p>
<p>If <code>partition_strategy</code> is <code>"div"</code>, we assign ids to partitions in a
contiguous manner. In this case, 13 ids are split across 5 partitions as:
<code>[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</code></p>
<p>The results of the lookup are concatenated into a dense
tensor. The returned tensor has shape <code>shape(ids) + shape(params)[1:]</code>.</p>
<p>Args:
  params: A list of tensors with the same type and which can be concatenated
    along dimension 0. Each <code>Tensor</code> must be appropriately sized for the given
    <code>partition_strategy</code>.
  ids: A <code>Tensor</code> with type <code>int32</code> or <code>int64</code> containing the ids to be looked
    up in <code>params</code>.
  partition_strategy: A string specifying the partitioning strategy, relevant
    if <code>len(params) &gt; 1</code>. Currently <code>"div"</code> and <code>"mod"</code> are supported. Default
    is <code>"mod"</code>.
  name: A name for the operation (optional).
  validate_indices: Whether or not to validate gather indices.</p>
<p>Returns:
  A <code>Tensor</code> with the same type as the tensors in <code>params</code>.</p>
<p>Raises:
  ValueError: If <code>params</code> is empty.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_embedding_lookup_sparse">
    <p>def <span class="ident">map_embedding_lookup_sparse</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.embedding_lookup_sparse</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.embedding_lookup_sparse</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.embedding_lookup_sparse can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.embedding_lookup_sparse</code> </strong></p>
<div class="codehilite"><pre><span></span>    def embedding_lookup_sparse(params, sp_ids, sp_weights, partition_strategy=&quot;mod&quot;, name=None, combiner=&quot;mean&quot;)
</pre></div>


<p>Computes embeddings for the given ids and weights.</p>
<p>This op assumes that there is at least one id for each row in the dense tensor
represented by sp_ids (i.e. there are no rows with empty features), and that
all the indices of sp_ids are in canonical row-major order.</p>
<p>It also assumes that all id values lie in the range [0, p0), where p0
is the sum of the size of params along dimension 0.</p>
<p>Args:
  params: A single tensor representing the complete embedding tensor,
    or a list of P tensors all of same shape except for the first dimension,
    representing sharded embedding tensors.
  sp_ids: N x M SparseTensor of int64 ids (typically from FeatureValueToId),
    where N is typically batch size and M is arbitrary.
  sp_weights: either a SparseTensor of float / double weights, or None to
    indicate all weights should be taken to be 1. If specified, sp_weights
    must have exactly the same shape and indices as sp_ids.
  partition_strategy: A string specifying the partitioning strategy, relevant
    if <code>len(params) &gt; 1</code>. Currently <code>"div"</code> and <code>"mod"</code> are supported. Default
    is <code>"mod"</code>. See <code>tf.nn.embedding_lookup</code> for more details.
  name: Optional name for the op.
  combiner: A string specifying the reduction op. Currently "mean", "sqrtn"
    and "sum" are supported.
    "sum" computes the weighted sum of the embedding results for each row.
    "mean" is the weighted sum divided by the total weight.
    "sqrtn" is the weighted sum divided by the square root of the sum of the
    squares of the weights.</p>
<p>Returns:
  A dense tensor representing the combined embeddings for the
  sparse ids. For each row in the dense tensor represented by sp_ids, the op
  looks up the embeddings for all ids in that row, multiplies them by the
  corresponding weight, and combines these embeddings as specified.</p>
<p>In other words, if
    shape(combined params) = [p0, p1, ..., pm]
  and
    shape(sp_ids) = shape(sp_weights) = [d0, d1, ..., dn]
  then
    shape(output) = [d0, d1, ..., dn-1, p1, ..., pm].</p>
<p>For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are</p>
<div class="codehilite"><pre><span></span>[0, 0]: id 1, weight 2.0
[0, 1]: id 3, weight 0.5
[1, 0]: id 0, weight 1.0
[2, 3]: id 1, weight 3.0
</pre></div>


<p>with combiner="mean", then the output will be a 3x20 matrix where
    output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)
    output[1, :] = params[0, :] * 1.0
    output[2, :] = params[1, :] * 3.0</p>
<p>Raises:
  TypeError: If sp_ids is not a SparseTensor, or if sp_weights is neither
    None nor SparseTensor.
  ValueError: If combiner is not one of {"mean", "sqrtn", "sum"}.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_fixed_unigram_candidate_sampler">
    <p>def <span class="ident">map_fixed_unigram_candidate_sampler</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.fixed_unigram_candidate_sampler</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.fixed_unigram_candidate_sampler</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.fixed_unigram_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.fixed_unigram_candidate_sampler</code> </strong></p>
<div class="codehilite"><pre><span></span>    def fixed_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, vocab_file=&quot;&quot;, distortion=1.0, num_reserved_ids=0, num_shards=1, shard=0, unigrams=(), seed=None, name=None)
</pre></div>


<p>Samples a set of classes using the provided (fixed) base distribution.</p>
<p>This operation randomly samples a tensor of sampled classes
(<code>sampled_candidates</code>) from the range of integers <code>[0, range_max]</code>.</p>
<p>The elements of <code>sampled_candidates</code> are drawn without replacement
(if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from
the base distribution.</p>
<p>The base distribution is read from a file or passed in as an
in-memory array. There is also an option to skew the distribution by
applying a distortion power to the weights.</p>
<p>In addition, this operation returns tensors <code>true_expected_count</code>
and <code>sampled_expected_count</code> representing the number of times each
of the target classes (<code>true_classes</code>) and the sampled
classes (<code>sampled_candidates</code>) is expected to occur in an average
tensor of sampled classes.  These values correspond to <code>Q(y|x)</code>
defined in <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">this
document</a>.
If <code>unique=True</code>, then these are post-rejection probabilities and we
compute them approximately.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  unique: A <code>bool</code>. Determines whether all sampled classes in a batch are
    unique.
  range_max: An <code>int</code>. The number of possible classes.
  vocab_file: Each valid line in this file (which should have a CSV-like
    format) corresponds to a valid word ID. IDs are in sequential order,
    starting from num_reserved_ids. The last entry in each line is expected
    to be a value corresponding to the count or relative probability. Exactly
    one of <code>vocab_file</code> and <code>unigrams</code> needs to be passed to this operation.
  distortion: The distortion is used to skew the unigram probability
    distribution.  Each weight is first raised to the distortion's power
    before adding to the internal unigram distribution. As a result,
    <code>distortion = 1.0</code> gives regular unigram sampling (as defined by the vocab
    file), and <code>distortion = 0.0</code> gives a uniform distribution.
  num_reserved_ids: Optionally some reserved IDs can be added in the range
    <code>[0, num_reserved_ids]</code> by the users. One use case is that a special
    unknown word token is used as ID 0. These IDs will have a sampling
    probability of 0.
  num_shards: A sampler can be used to sample from a subset of the original
    range in order to speed up the whole computation through parallelism. This
    parameter (together with <code>shard</code>) indicates the number of partitions that
    are being used in the overall computation.
  shard: A sampler can be used to sample from a subset of the original range
    in order to speed up the whole computation through parallelism. This
    parameter (together with <code>num_shards</code>) indicates the particular partition
    number of the operation, when partitioning is being used.
  unigrams: A list of unigram counts or probabilities, one per ID in
    sequential order. Exactly one of <code>vocab_file</code> and <code>unigrams</code> should be
    passed to this operation.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled classes.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_in_top_k">
    <p>def <span class="ident">map_in_top_k</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.in_top_k</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.in_top_k</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.in_top_k can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.in_top_k</code> </strong></p>
<div class="codehilite"><pre><span></span>    def in_top_k(predictions, targets, k, name=None)
</pre></div>


<p>Says whether the targets are in the top <code>K</code> predictions.</p>
<p>This outputs a <code>batch_size</code> bool array, an entry <code>out[i]</code> is <code>true</code> if the
prediction for the target class is among the top <code>k</code> predictions among
all predictions for example <code>i</code>. Note that the behavior of <code>InTopK</code> differs
from the <code>TopK</code> op in its handling of ties; if multiple classes have the
same prediction value and straddle the top-<code>k</code> boundary, all of those
classes are considered to be in the top <code>k</code>.</p>
<p>More formally, let</p>
<p>(predictions_i) be the predictions for all classes for example <code>i</code>,
  (targets_i) be the target class for example <code>i</code>,
  (out_i) be the output for example <code>i</code>,</p>
<p>$$out_i = predictions_{i, targets_i} \in TopKIncludingTies(predictions_i)$$</p>
<p>Args:
  predictions: A <code>Tensor</code> of type <code>float32</code>.
    A <code>batch_size</code> x <code>classes</code> tensor.
  targets: A <code>Tensor</code>. Must be one of the following types: <code>int32</code>, <code>int64</code>.
    A <code>batch_size</code> vector of class ids.
  k: An <code>int</code>. Number of top elements to look at for computing precision.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of type <code>bool</code>. Computed Precision at <code>k</code> as a <code>bool Tensor</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_l2_loss">
    <p>def <span class="ident">map_l2_loss</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.l2_loss</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.l2_loss</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.l2_loss can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.l2_loss</code> </strong></p>
<div class="codehilite"><pre><span></span>    def l2_loss(t, name=None)
</pre></div>


<p>L2 Loss.</p>
<p>Computes half the L2 norm of a tensor without the <code>sqrt</code>:</p>
<div class="codehilite"><pre><span></span>output = sum(t ** 2) / 2
</pre></div>


<p>Args:
  t: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int64</code>, <code>int32</code>, <code>uint8</code>, <code>uint16</code>, <code>int16</code>, <code>int8</code>, <code>complex64</code>, <code>complex128</code>, <code>qint8</code>, <code>quint8</code>, <code>qint32</code>, <code>half</code>.
    Typically 2-D, but may have any dimensions.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>t</code>. 0-D.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_l2_normalize">
    <p>def <span class="ident">map_l2_normalize</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.l2_normalize</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.l2_normalize</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.l2_normalize can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.l2_normalize</code> </strong></p>
<div class="codehilite"><pre><span></span>    def l2_normalize(x, dim, epsilon=1e-12, name=None)
</pre></div>


<p>Normalizes along dimension <code>dim</code> using an L2 norm.</p>
<p>For a 1-D tensor with <code>dim = 0</code>, computes</p>
<div class="codehilite"><pre><span></span>output = x / sqrt(max(sum(x**2), epsilon))
</pre></div>


<p>For <code>x</code> with more dimensions, independently normalizes each 1-D slice along
dimension <code>dim</code>.</p>
<p>Args:
  x: A <code>Tensor</code>.
  dim: Dimension along which to normalize.
  epsilon: A lower bound value for the norm. Will use <code>sqrt(epsilon)</code> as the
    divisor if <code>norm &lt; sqrt(epsilon)</code>.
  name: A name for this operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> with the same shape as <code>x</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_learned_unigram_candidate_sampler">
    <p>def <span class="ident">map_learned_unigram_candidate_sampler</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.learned_unigram_candidate_sampler</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.learned_unigram_candidate_sampler</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.learned_unigram_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.learned_unigram_candidate_sampler</code> </strong></p>
<div class="codehilite"><pre><span></span>    def learned_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)
</pre></div>


<p>Samples a set of classes from a distribution learned during training.</p>
<p>This operation randomly samples a tensor of sampled classes
(<code>sampled_candidates</code>) from the range of integers <code>[0, range_max]</code>.</p>
<p>The elements of <code>sampled_candidates</code> are drawn without replacement
(if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from
the base distribution.</p>
<p>The base distribution for this operation is constructed on the fly
during training.  It is a unigram distribution over the target
classes seen so far during training.  Every integer in <code>[0, range_max]</code>
begins with a weight of 1, and is incremented by 1 each time it is
seen as a target class.  The base distribution is not saved to checkpoints,
so it is reset when the model is reloaded.</p>
<p>In addition, this operation returns tensors <code>true_expected_count</code>
and <code>sampled_expected_count</code> representing the number of times each
of the target classes (<code>true_classes</code>) and the sampled
classes (<code>sampled_candidates</code>) is expected to occur in an average
tensor of sampled classes.  These values correspond to <code>Q(y|x)</code>
defined in <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">this
document</a>.
If <code>unique=True</code>, then these are post-rejection probabilities and we
compute them approximately.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  unique: A <code>bool</code>. Determines whether all sampled classes in a batch are
    unique.
  range_max: An <code>int</code>. The number of possible classes.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled classes.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_local_response_normalization">
    <p>def <span class="ident">map_local_response_normalization</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.local_response_normalization</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.local_response_normalization</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.local_response_normalization can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.local_response_normalization</code> </strong></p>
<div class="codehilite"><pre><span></span>    def lrn(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None)
</pre></div>


<p>Local Response Normalization.</p>
<p>The 4-D <code>input</code> tensor is treated as a 3-D array of 1-D vectors (along the last
dimension), and each vector is normalized independently.  Within a given vector,
each component is divided by the weighted, squared sum of inputs within
<code>depth_radius</code>.  In detail,</p>
<div class="codehilite"><pre><span></span>sqr_sum[a, b, c, d] =
    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
</pre></div>


<p>For details, see [Krizhevsky et al., ImageNet classification with deep
convolutional neural networks (NIPS 2012)]
(http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).</p>
<p>Args:
  input: A <code>Tensor</code> of type <code>float32</code>. 4-D.
  depth_radius: An optional <code>int</code>. Defaults to <code>5</code>.
    0-D.  Half-width of the 1-D normalization window.
  bias: An optional <code>float</code>. Defaults to <code>1</code>.
    An offset (usually positive to avoid dividing by 0).
  alpha: An optional <code>float</code>. Defaults to <code>1</code>.
    A scale factor, usually positive.
  beta: An optional <code>float</code>. Defaults to <code>0.5</code>. An exponent.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of type <code>float32</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_log_softmax">
    <p>def <span class="ident">map_log_softmax</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.log_softmax</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.log_softmax</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.log_softmax can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.log_softmax</code> </strong></p>
<div class="codehilite"><pre><span></span>    def log_softmax(logits, name=None)
</pre></div>


<p>Computes log softmax activations.</p>
<p>For each batch <code>i</code> and class <code>j</code> we have</p>
<div class="codehilite"><pre><span></span>logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))
</pre></div>


<p>Args:
  logits: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    2-D with shape <code>[batch_size, num_classes]</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>logits</code>. Same shape as <code>logits</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_log_uniform_candidate_sampler">
    <p>def <span class="ident">map_log_uniform_candidate_sampler</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.log_uniform_candidate_sampler</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.log_uniform_candidate_sampler</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.log_uniform_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.log_uniform_candidate_sampler</code> </strong></p>
<div class="codehilite"><pre><span></span>    def log_uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)
</pre></div>


<p>Samples a set of classes using a log-uniform (Zipfian) base distribution.</p>
<p>This operation randomly samples a tensor of sampled classes
(<code>sampled_candidates</code>) from the range of integers <code>[0, range_max]</code>.</p>
<p>The elements of <code>sampled_candidates</code> are drawn without replacement
(if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from
the base distribution.</p>
<p>The base distribution for this operation is an approximately log-uniform
or Zipfian distribution:</p>
<p><code>P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)</code></p>
<p>This sampler is useful when the target classes approximately follow such
a distribution - for example, if the classes represent words in a lexicon
sorted in decreasing order of frequency. If your classes are not ordered by
decreasing frequency, do not use this op.</p>
<p>In addition, this operation returns tensors <code>true_expected_count</code>
and <code>sampled_expected_count</code> representing the number of times each
of the target classes (<code>true_classes</code>) and the sampled
classes (<code>sampled_candidates</code>) is expected to occur in an average
tensor of sampled classes.  These values correspond to <code>Q(y|x)</code>
defined in <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">this
document</a>.
If <code>unique=True</code>, then these are post-rejection probabilities and we
compute them approximately.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  unique: A <code>bool</code>. Determines whether all sampled classes in a batch are
    unique.
  range_max: An <code>int</code>. The number of possible classes.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled classes.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_lrn">
    <p>def <span class="ident">map_lrn</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.lrn</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.lrn</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.lrn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.lrn</code> </strong></p>
<div class="codehilite"><pre><span></span>    def lrn(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None)
</pre></div>


<p>Local Response Normalization.</p>
<p>The 4-D <code>input</code> tensor is treated as a 3-D array of 1-D vectors (along the last
dimension), and each vector is normalized independently.  Within a given vector,
each component is divided by the weighted, squared sum of inputs within
<code>depth_radius</code>.  In detail,</p>
<div class="codehilite"><pre><span></span>sqr_sum[a, b, c, d] =
    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
</pre></div>


<p>For details, see [Krizhevsky et al., ImageNet classification with deep
convolutional neural networks (NIPS 2012)]
(http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).</p>
<p>Args:
  input: A <code>Tensor</code> of type <code>float32</code>. 4-D.
  depth_radius: An optional <code>int</code>. Defaults to <code>5</code>.
    0-D.  Half-width of the 1-D normalization window.
  bias: An optional <code>float</code>. Defaults to <code>1</code>.
    An offset (usually positive to avoid dividing by 0).
  alpha: An optional <code>float</code>. Defaults to <code>1</code>.
    A scale factor, usually positive.
  beta: An optional <code>float</code>. Defaults to <code>0.5</code>. An exponent.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of type <code>float32</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_make_all">
    <p>def <span class="ident">map_make_all</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.make_all</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.make_all</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.make_all can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.make_all</code> </strong></p>
<div class="codehilite"><pre><span></span>    def make_all(module_name, doc_string_modules=None)
</pre></div>


<p>Generate <code>__all__</code> from the docstring of one or more modules.</p>
<p>Usage: <code>make_all(__name__)</code> or
<code>make_all(__name__, [sys.modules(__name__), other_module])</code>. The doc string
modules must each a docstring, and <code>__all__</code> will contain all symbols with
<code>@@</code> references, where that symbol currently exists in the module named
<code>module_name</code>.</p>
<p>Args:
  module_name: The name of the module (usually <code>__name__</code>).
  doc_string_modules: a list of modules from which to take docstring.
  If None, then a list containing only the module named <code>module_name</code> is used.</p>
<p>Returns:
  A list suitable for use as <code>__all__</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_max_pool">
    <p>def <span class="ident">map_max_pool</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.max_pool</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.max_pool</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.max_pool can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.max_pool</code> </strong></p>
<div class="codehilite"><pre><span></span>    def max_pool(value, ksize, strides, padding, data_format=&quot;NHWC&quot;, name=None)
</pre></div>


<p>Performs the max pooling on the input.</p>
<p>Args:
  value: A 4-D <code>Tensor</code> with shape <code>[batch, height, width, channels]</code> and
    type <code>tf.float32</code>.
  ksize: A list of ints that has length &gt;= 4.  The size of the window for
    each dimension of the input tensor.
  strides: A list of ints that has length &gt;= 4.  The stride of the sliding
    window for each dimension of the input tensor.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm.
  data_format: A string. 'NHWC' and 'NCHW' are supported.
  name: Optional name for the operation.</p>
<p>Returns:
  A <code>Tensor</code> with type <code>tf.float32</code>.  The max pooled output tensor.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_max_pool_with_argmax">
    <p>def <span class="ident">map_max_pool_with_argmax</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.max_pool_with_argmax</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.max_pool_with_argmax</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.max_pool_with_argmax can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.max_pool_with_argmax</code> </strong></p>
<div class="codehilite"><pre><span></span>    def max_pool_with_argmax(input, ksize, strides, padding, Targmax=None, name=None)
</pre></div>


<p>Performs max pooling on the input and outputs both max values and indices.</p>
<p>The indices in <code>argmax</code> are flattened, so that a maximum value at position
<code>[b, y, x, c]</code> becomes flattened index
<code>((b * height + y) * width + x) * channels + c</code>.</p>
<p>Args:
  input: A <code>Tensor</code> of type <code>float32</code>.
    4-D with shape <code>[batch, height, width, channels]</code>.  Input to pool over.
  ksize: A list of <code>ints</code> that has length <code>&gt;= 4</code>.
    The size of the window for each dimension of the input tensor.
  strides: A list of <code>ints</code> that has length <code>&gt;= 4</code>.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  Targmax: An optional <code>tf.DType</code> from: <code>tf.int32, tf.int64</code>. Defaults to <code>tf.int64</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A tuple of <code>Tensor</code> objects (output, argmax).
  output: A <code>Tensor</code> of type <code>float32</code>. The max pooled output tensor.
  argmax: A <code>Tensor</code> of type <code>Targmax</code>. 4-D.  The flattened indices of the max values chosen for each output.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_moments">
    <p>def <span class="ident">map_moments</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.moments</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.moments</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.moments can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.moments</code> </strong></p>
<div class="codehilite"><pre><span></span>    def moments(x, axes, name=None, keep_dims=False)
</pre></div>


<p>Calculate the mean and variance of <code>x</code>.</p>
<p>The mean and variance are calculated by aggregating the contents of <code>x</code>
across <code>axes</code>.  If <code>x</code> is 1-D and <code>axes = [0]</code> this is just the mean
and variance of a vector.</p>
<p>When using these moments for batch normalization (see
<code>tf.nn.batch_normalization</code>):
  * for so-called "global normalization", used with convolutional filters with
    shape <code>[batch, height, width, depth]</code>, pass <code>axes=[0, 1, 2]</code>.
  * for simple batch normalization pass <code>axes=[0]</code> (batch only).</p>
<p>Args:
  x: A <code>Tensor</code>.
  axes: array of ints.  Axes along which to compute mean and
    variance.
  keep_dims: produce moments with the same dimensionality as the input.
  name: Name used to scope the operations that compute the moments.</p>
<p>Returns:
  Two <code>Tensor</code> objects: <code>mean</code> and <code>variance</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_nce_loss">
    <p>def <span class="ident">map_nce_loss</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.nce_loss</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.nce_loss</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.nce_loss can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.nce_loss</code> </strong></p>
<div class="codehilite"><pre><span></span>    def nce_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, partition_strategy=&quot;mod&quot;, name=&quot;nce_loss&quot;)
</pre></div>


<p>Computes and returns the noise-contrastive estimation training loss.</p>
<p>See [Noise-contrastive estimation: A new estimation principle for
unnormalized statistical models]
(http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf).
Also see our [Candidate Sampling Algorithms Reference]
(../../extras/candidate_sampling.pdf)</p>
<p>Note: In the case where <code>num_true</code> &gt; 1, we assign to each target class
the target probability 1 / <code>num_true</code> so that the target probabilities
sum to 1 per-example.</p>
<p>Note: It would be useful to allow a variable number of target classes per
example.  We hope to provide this functionality in a future release.
For now, if you have a variable number of target classes, you can pad them
out to a constant number by either repeating them or by padding
with an otherwise unused class.</p>
<p>Args:
  weights: A <code>Tensor</code> of shape <code>[num_classes, dim]</code>, or a list of <code>Tensor</code>
      objects whose concatenation along dimension 0 has shape
      [num_classes, dim].  The (possibly-partitioned) class embeddings.
  biases: A <code>Tensor</code> of shape <code>[num_classes]</code>.  The class biases.
  inputs: A <code>Tensor</code> of shape <code>[batch_size, dim]</code>.  The forward
      activations of the input network.
  labels: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
      num_true]</code>. The target classes.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  num_classes: An <code>int</code>. The number of possible classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  sampled_values: a tuple of (<code>sampled_candidates</code>, <code>true_expected_count</code>,
      <code>sampled_expected_count</code>) returned by a <code>*_candidate_sampler</code> function.
      (if None, we default to <code>log_uniform_candidate_sampler</code>)
  remove_accidental_hits:  A <code>bool</code>.  Whether to remove "accidental hits"
      where a sampled class equals one of the target classes.  If set to
      <code>True</code>, this is a "Sampled Logistic" loss instead of NCE, and we are
      learning to generate log-odds instead of log probabilities.  See
      our [Candidate Sampling Algorithms Reference]
      (../../extras/candidate_sampling.pdf).
      Default is False.
  partition_strategy: A string specifying the partitioning strategy, relevant
      if <code>len(weights) &gt; 1</code>. Currently <code>"div"</code> and <code>"mod"</code> are supported.
      Default is <code>"mod"</code>. See <code>tf.nn.embedding_lookup</code> for more details.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>batch_size</code> 1-D tensor of per-example NCE losses.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_normalize_moments">
    <p>def <span class="ident">map_normalize_moments</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.normalize_moments</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.normalize_moments</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.normalize_moments can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.normalize_moments</code> </strong></p>
<div class="codehilite"><pre><span></span>    def normalize_moments(counts, mean_ss, variance_ss, shift, name=None)
</pre></div>


<p>Calculate the mean and variance of based on the sufficient statistics.</p>
<p>Args:
  counts: A <code>Tensor</code> containing a the total count of the data (one value).
  mean_ss: A <code>Tensor</code> containing the mean sufficient statistics: the (possibly
    shifted) sum of the elements to average over.
  variance_ss: A <code>Tensor</code> containing the variance sufficient statistics: the
    (possibly shifted) squared sum of the data to compute the variance over.
  shift: A <code>Tensor</code> containing the value by which the data is shifted for
    numerical stability, or <code>None</code> if no shift was performed.
  name: Name used to scope the operations that compute the moments.</p>
<p>Returns:
  Two <code>Tensor</code> objects: <code>mean</code> and <code>variance</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_relu">
    <p>def <span class="ident">map_relu</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.relu</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.relu</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.relu can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.relu</code> </strong></p>
<div class="codehilite"><pre><span></span>    def relu(features, name=None)
</pre></div>


<p>Computes rectified linear: <code>max(features, 0)</code>.</p>
<p>Args:
  features: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_relu6">
    <p>def <span class="ident">map_relu6</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.relu6</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.relu6</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.relu6 can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.relu6</code> </strong></p>
<div class="codehilite"><pre><span></span>    def relu6(features, name=None)
</pre></div>


<p>Computes Rectified Linear 6: <code>min(max(features, 0), 6)</code>.</p>
<p>Args:
  features: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>,
    <code>int16</code>, or <code>int8</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_relu_layer">
    <p>def <span class="ident">map_relu_layer</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.relu_layer</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.relu_layer</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.relu_layer can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.relu_layer</code> </strong></p>
<div class="codehilite"><pre><span></span>    def relu_layer(x, weights, biases, name=None)
</pre></div>


<p>Computes Relu(x * weight + biases).</p>
<p>Args:
  x: a 2D tensor.  Dimensions typically: batch, in_units
  weights: a 2D tensor.  Dimensions typically: in_units, out_units
  biases: a 1D tensor.  Dimensions: out_units
  name: A name for the operation (optional).  If not specified
    "nn_relu_layer" is used.</p>
<p>Returns:
  A 2-D Tensor computing relu(matmul(x, weights) + biases).
  Dimensions typically: batch, out_units.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_rnn">
    <p>def <span class="ident">map_rnn</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.rnn</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.rnn</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.rnn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.rnn</code> </strong></p>
<div class="codehilite"><pre><span></span>    def rnn(cell, inputs, initial_state=None, dtype=None, sequence_length=None, scope=None)
</pre></div>


<p>Creates a recurrent neural network specified by RNNCell "cell".</p>
<p>The simplest form of RNN network generated is:
  state = cell.zero_state(...)
  outputs = []
  for input_ in inputs:
    output, state = cell(input_, state)
    outputs.append(output)
  return (outputs, state)</p>
<p>However, a few other options are available:</p>
<p>An initial state can be provided.
If the sequence_length vector is provided, dynamic calculation is performed.
This method of calculation does not compute the RNN steps past the maximum
sequence length of the minibatch (thus saving computational time),
and properly propagates the state at an example's sequence length
to the final state output.</p>
<p>The dynamic calculation performed is, at time t for batch row b,
  (output, state)(b, t) =
    (t &gt;= sequence_length(b))
      ? (zeros(cell.output_size), states(b, sequence_length(b) - 1))
      : cell(input(b, t), state(b, t - 1))</p>
<p>Args:
  cell: An instance of RNNCell.
  inputs: A length T list of inputs, each a tensor of shape
    [batch_size, cell.input_size].
  initial_state: (optional) An initial state for the RNN.  This must be
    a tensor of appropriate type and shape [batch_size x cell.state_size].
  dtype: (optional) The data type for the initial state.  Required if
    initial_state is not provided.
  sequence_length: Specifies the length of each sequence in inputs.
    An int32 or int64 vector (tensor) size [batch_size].  Values in [0, T).
  scope: VariableScope for the created subgraph; defaults to "RNN".</p>
<p>Returns:
  A pair (outputs, state) where:
    outputs is a length T list of outputs (one for each input)
    state is the final state</p>
<p>Raises:
  TypeError: If "cell" is not an instance of RNNCell.
  ValueError: If inputs is None or an empty list, or if the input depth
    cannot be inferred from inputs via shape inference.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_sampled_softmax_loss">
    <p>def <span class="ident">map_sampled_softmax_loss</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.sampled_softmax_loss</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.sampled_softmax_loss</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sampled_softmax_loss can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sampled_softmax_loss</code> </strong></p>
<div class="codehilite"><pre><span></span>    def sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy=&quot;mod&quot;, name=&quot;sampled_softmax_loss&quot;)
</pre></div>


<p>Computes and returns the sampled softmax training loss.</p>
<p>This is a faster way to train a softmax classifier over a huge number of
classes.</p>
<p>This operation is for training only.  It is generally an underestimate of
the full softmax loss.</p>
<p>At inference time, you can compute full softmax probabilities with the
expression <code>tf.nn.softmax(tf.matmul(inputs, weights) + biases)</code>.</p>
<p>See our [Candidate Sampling Algorithms Reference]
(../../extras/candidate_sampling.pdf)</p>
<p>Also see Section 3 of <a href="http://arxiv.org/abs/1412.2007">Jean et al., 2014</a>
(<a href="http://arxiv.org/pdf/1412.2007.pdf">pdf</a>) for the math.</p>
<p>Args:
  weights: A <code>Tensor</code> of shape <code>[num_classes, dim]</code>, or a list of <code>Tensor</code>
      objects whose concatenation along dimension 0 has shape
      [num_classes, dim].  The (possibly-sharded) class embeddings.
  biases: A <code>Tensor</code> of shape <code>[num_classes]</code>.  The class biases.
  inputs: A <code>Tensor</code> of shape <code>[batch_size, dim]</code>.  The forward
      activations of the input network.
  labels: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
      num_true]</code>. The target classes.  Note that this format differs from
      the <code>labels</code> argument of <code>nn.softmax_cross_entropy_with_logits</code>.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  num_classes: An <code>int</code>. The number of possible classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  sampled_values: a tuple of (<code>sampled_candidates</code>, <code>true_expected_count</code>,
      <code>sampled_expected_count</code>) returned by a <code>*_candidate_sampler</code> function.
      (if None, we default to <code>log_uniform_candidate_sampler</code>)
  remove_accidental_hits:  A <code>bool</code>.  whether to remove "accidental hits"
      where a sampled class equals one of the target classes.  Default is
      True.
  partition_strategy: A string specifying the partitioning strategy, relevant
      if <code>len(weights) &gt; 1</code>. Currently <code>"div"</code> and <code>"mod"</code> are supported.
      Default is <code>"mod"</code>. See <code>tf.nn.embedding_lookup</code> for more details.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>batch_size</code> 1-D tensor of per-example sampled softmax losses.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_separable_conv2d">
    <p>def <span class="ident">map_separable_conv2d</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.separable_conv2d</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.separable_conv2d</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.separable_conv2d can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.separable_conv2d</code> </strong></p>
<div class="codehilite"><pre><span></span>    def separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=None)
</pre></div>


<p>2-D convolution with separable filters.</p>
<p>Performs a depthwise convolution that acts separately on channels followed by
a pointwise convolution that mixes channels.  Note that this is separability
between dimensions <code>[1, 2]</code> and <code>3</code>, not spatial separability between
dimensions <code>1</code> and <code>2</code>.</p>
<p>In detail,</p>
<div class="codehilite"><pre><span></span>output[b, i, j, k] = sum_{di, dj, q, r]
    input[b, strides[1] * i + di, strides[2] * j + dj, q] *
    depthwise_filter[di, dj, q, r] *
    pointwise_filter[0, 0, q * channel_multiplier + r, k]
</pre></div>


<p><code>strides</code> controls the strides for the depthwise convolution only, since
the pointwise convolution has implicit strides of <code>[1, 1, 1, 1]</code>.  Must have
<code>strides[0] = strides[3] = 1</code>.  For the most common case of the same
horizontal and vertical strides, <code>strides = [1, stride, stride, 1]</code>.</p>
<p>Args:
  input: 4-D <code>Tensor</code> with shape <code>[batch, in_height, in_width, in_channels]</code>.
  depthwise_filter: 4-D <code>Tensor</code> with shape
    <code>[filter_height, filter_width, in_channels, channel_multiplier]</code>.
    Contains <code>in_channels</code> convolutional filters of depth 1.
  pointwise_filter: 4-D <code>Tensor</code> with shape
    <code>[1, 1, channel_multiplier * in_channels, out_channels]</code>.  Pointwise
    filter to mix channels after <code>depthwise_filter</code> has convolved spatially.
  strides: 1-D of size 4.  The strides for the depthwise convolution for
    each dimension of <code>input</code>.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>.  The padding algorithm.
  name: A name for this operation (optional).</p>
<p>Returns:
  A 4-D <code>Tensor</code> of shape <code>[batch, out_height, out_width, out_channels]</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_sigmoid">
    <p>def <span class="ident">map_sigmoid</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.sigmoid</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.sigmoid</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sigmoid can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sigmoid</code> </strong></p>
<div class="codehilite"><pre><span></span>    def sigmoid(x, name=None)
</pre></div>


<p>Computes sigmoid of <code>x</code> element-wise.</p>
<p>Specifically, <code>y = 1 / (1 + exp(-x))</code>.</p>
<p>Args:
  x: A Tensor with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>complex64</code>, <code>int64</code>,
    or <code>qint32</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A Tensor with the same type as <code>x</code> if <code>x.dtype != qint32</code>
    otherwise the return type is <code>quint8</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_sigmoid_cross_entropy_with_logits">
    <p>def <span class="ident">map_sigmoid_cross_entropy_with_logits</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.sigmoid_cross_entropy_with_logits</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.sigmoid_cross_entropy_with_logits</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sigmoid_cross_entropy_with_logits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sigmoid_cross_entropy_with_logits</code> </strong></p>
<div class="codehilite"><pre><span></span>    def sigmoid_cross_entropy_with_logits(logits, targets, name=None)
</pre></div>


<p>Computes sigmoid cross entropy given <code>logits</code>.</p>
<p>Measures the probability error in discrete classification tasks in which each
class is independent and not mutually exclusive.  For instance, one could
perform multilabel classification where a picture can contain both an elephant
and a dog at the same time.</p>
<p>For brevity, let <code>x = logits</code>, <code>z = targets</code>.  The logistic loss is</p>
<div class="codehilite"><pre><span></span>  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + log(1 + exp(-x))
= x - x * z + log(1 + exp(-x))
</pre></div>


<p>To ensure stability and avoid overflow, the implementation uses</p>
<div class="codehilite"><pre><span></span>max(x, 0) - x * z + log(1 + exp(-abs(x)))
</pre></div>


<p><code>logits</code> and <code>targets</code> must have the same type and shape.</p>
<p>Args:
  logits: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.
  targets: A <code>Tensor</code> of the same type and shape as <code>logits</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of the same shape as <code>logits</code> with the componentwise
  logistic losses.</p>
<p>Raises:
  ValueError: If <code>logits</code> and <code>targets</code> do not have the same shape.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_softmax">
    <p>def <span class="ident">map_softmax</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.softmax</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.softmax</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.softmax can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.softmax</code> </strong></p>
<div class="codehilite"><pre><span></span>    def softmax(logits, name=None)
</pre></div>


<p>Computes softmax activations.</p>
<p>For each batch <code>i</code> and class <code>j</code> we have</p>
<div class="codehilite"><pre><span></span>softmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))
</pre></div>


<p>Args:
  logits: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    2-D with shape <code>[batch_size, num_classes]</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>logits</code>. Same shape as <code>logits</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_softmax_cross_entropy_with_logits">
    <p>def <span class="ident">map_softmax_cross_entropy_with_logits</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.softmax_cross_entropy_with_logits</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.softmax_cross_entropy_with_logits</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.softmax_cross_entropy_with_logits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.softmax_cross_entropy_with_logits</code> </strong></p>
<div class="codehilite"><pre><span></span>    def softmax_cross_entropy_with_logits(logits, labels, name=None)
</pre></div>


<p>Computes softmax cross entropy between <code>logits</code> and <code>labels</code>.</p>
<p>Measures the probability error in discrete classification tasks in which the
classes are mutually exclusive (each entry is in exactly one class).  For
example, each CIFAR-10 image is labeled with one and only one label: an image
can be a dog or a truck, but not both.</p>
<p><strong>NOTE:</strong>  While the classes are mutually exclusive, their probabilities
need not be. If using exclusive <code>labels</code> (wherein one and only one class is
true at a time), see <code>sparse_softmax_cross_entropy_with_logits</code>.</p>
<p><strong>WARNING:</strong> This op expects unscaled logits, since it performs a <code>softmax</code>
on <code>logits</code> internally for efficiency.  Do not call this op with the
output of <code>softmax</code>, as it will produce incorrect results.</p>
<p><code>logits</code> and <code>labels</code> must have the same shape <code>[batch_size, num_classes]</code>
and the same dtype (either <code>float32</code> or <code>float64</code>).</p>
<p>Args:
  logits: Unscaled log probabilities.
  labels: Each row <code>labels[i]</code> must be a valid probability distribution or
      all zeros. If all zeros, the corresponding loss will be <code>0</code>, regardless
      of the contents of <code>logits[i]</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A 1-D <code>Tensor</code> of length <code>batch_size</code> of the same type as <code>logits</code> with the
  softmax cross entropy loss.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_softplus">
    <p>def <span class="ident">map_softplus</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.softplus</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.softplus</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.softplus can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.softplus</code> </strong></p>
<div class="codehilite"><pre><span></span>    def softplus(features, name=None)
</pre></div>


<p>Computes softplus: <code>log(exp(features) + 1)</code>.</p>
<p>Args:
  features: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_softsign">
    <p>def <span class="ident">map_softsign</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.softsign</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.softsign</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.softsign can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.softsign</code> </strong></p>
<div class="codehilite"><pre><span></span>    def softsign(features, name=None)
</pre></div>


<p>Computes softsign: <code>features / (abs(features) + 1)</code>.</p>
<p>Args:
  features: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_sparse_softmax_cross_entropy_with_logits">
    <p>def <span class="ident">map_sparse_softmax_cross_entropy_with_logits</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sparse_softmax_cross_entropy_with_logits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code> </strong></p>
<div class="codehilite"><pre><span></span>    def sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)
</pre></div>


<p>Computes sparse softmax cross entropy between <code>logits</code> and <code>labels</code>.</p>
<p>Measures the probability error in discrete classification tasks in which the
classes are mutually exclusive (each entry is in exactly one class).  For
example, each CIFAR-10 image is labeled with one and only one label: an image
can be a dog or a truck, but not both.</p>
<p><strong>NOTE:</strong>  For this operation, the probability of a given label is considered
exclusive.  That is, soft classes are not allowed, and the <code>labels</code> vector
must provide a single specific index for the true class for each row of
<code>logits</code> (each minibatch entry).  For soft softmax classification with
a probability distribution for each entry, see
<code>softmax_cross_entropy_with_logits</code>.</p>
<p><strong>WARNING:</strong> This op expects unscaled logits, since it performs a <code>softmax</code>
on <code>logits</code> internally for efficiency.  Do not call this op with the
output of <code>softmax</code>, as it will produce incorrect results.</p>
<p><code>logits</code> and must have the shape <code>[batch_size, num_classes]</code>
and the dtype (either <code>float32</code> or <code>float64</code>).</p>
<p><code>labels</code> must have the shape <code>[batch_size]</code> and the dtype <code>int64</code>.</p>
<p>Args:
  logits: Unscaled log probabilities.
  labels: Each entry <code>labels[i]</code> must be an index in <code>[0, num_classes)</code> or
      <code>-1</code>. If <code>-1</code>, the corresponding loss will be <code>0</code>, regardless
      of the contents of <code>logits[i]</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A 1-D <code>Tensor</code> of length <code>batch_size</code> of the same type as <code>logits</code> with the
  softmax cross entropy loss.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_state_saving_rnn">
    <p>def <span class="ident">map_state_saving_rnn</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.state_saving_rnn</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.state_saving_rnn</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.state_saving_rnn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.state_saving_rnn</code> </strong></p>
<div class="codehilite"><pre><span></span>    def state_saving_rnn(cell, inputs, state_saver, state_name, sequence_length=None, scope=None)
</pre></div>


<p>RNN that accepts a state saver for time-truncated RNN calculation.</p>
<p>Args:
  cell: An instance of RNNCell.
  inputs: A length T list of inputs, each a tensor of shape
    [batch_size, cell.input_size].
  state_saver: A state saver object with methods <code>state</code> and <code>save_state</code>.
  state_name: The name to use with the state_saver.
  sequence_length: (optional) An int32/int64 vector size [batch_size].
    See the documentation for rnn() for more details about sequence_length.
  scope: VariableScope for the created subgraph; defaults to "RNN".</p>
<p>Returns:
  A pair (outputs, state) where:
    outputs is a length T list of outputs (one for each input)
    states is the final state</p>
<p>Raises:
  TypeError: If "cell" is not an instance of RNNCell.
  ValueError: If inputs is None or an empty list.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_sufficient_statistics">
    <p>def <span class="ident">map_sufficient_statistics</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.sufficient_statistics</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.sufficient_statistics</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sufficient_statistics can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sufficient_statistics</code> </strong></p>
<div class="codehilite"><pre><span></span>    def sufficient_statistics(x, axes, shift=True, keep_dims=False, name=None)
</pre></div>


<p>Calculate the sufficient statistics for the mean and variance of <code>x</code>.</p>
<p>These sufficient statistics are computed using the one pass algorithm on
an input that's optionally shifted using the value of the 1st element in <code>x</code>.
See:
https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data</p>
<p>Args:
  x: A <code>Tensor</code>.
  axes: Array of ints. Axes along which to compute mean and variance.
  shift: If true, shift the data to provide more numerically stable results.
  keep_dims: produce statistics with the same dimensionality as the input.
  name: Name used to scope the operations that compute the sufficient stats.</p>
<p>Returns:
  Four <code>Tensor</code> objects of the same type as <code>x</code>:
  * the count (number of elements to average over).
  * the (possibly shifted) sum of the elements in the array.
  * the (possibly shifted) sum of squares of the elements in the array.
  * the shift by which the mean must be corrected or None if <code>shift</code> is False.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_tanh">
    <p>def <span class="ident">map_tanh</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.tanh</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.tanh</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.tanh can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.tanh</code> </strong></p>
<div class="codehilite"><pre><span></span>    def tanh(x, name=None)
</pre></div>


<p>Computes hyperbolic tangent of <code>x</code> element-wise.</p>
<p>Args:
  x: A Tensor with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>complex64</code>, <code>int64</code>,
    or <code>qint32</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A Tensor with the same type as <code>x</code> if <code>x.dtype != qint32</code> otherwise
    the return type is <code>quint8</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_top_k">
    <p>def <span class="ident">map_top_k</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.top_k</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.top_k</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.top_k can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.top_k</code> </strong></p>
<div class="codehilite"><pre><span></span>    def top_k(input, k=1, sorted=True, name=None)
</pre></div>


<p>Finds values and indices of the <code>k</code> largest entries for the last dimension.</p>
<p>If the input is a vector (rank-1), finds the <code>k</code> largest entries in the vector
and outputs their values and indices as vectors.  Thus <code>values[j]</code> is the
<code>j</code>-th largest entry in <code>input</code>, and its index is <code>indices[j]</code>.</p>
<p>For matrices (resp. higher rank input), computes the top <code>k</code> entries in each
row (resp. vector along the last dimension).  Thus,</p>
<div class="codehilite"><pre><span></span><span class="s s-Atom">values</span><span class="p">.</span><span class="s s-Atom">shape</span> <span class="o">=</span> <span class="s s-Atom">indices</span><span class="p">.</span><span class="s s-Atom">shape</span> <span class="o">=</span> <span class="s s-Atom">input</span><span class="p">.</span><span class="s s-Atom">shape</span><span class="p">[:-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s s-Atom">k</span><span class="p">]</span>
</pre></div>


<p>If two elements are equal, the lower-index element appears first.</p>
<p>Args:
  input: 1-D or higher <code>Tensor</code> with last dimension at least <code>k</code>.
  k: 0-D <code>int32</code> <code>Tensor</code>.  Number of top elements to look for along the last
    dimension (along each row for matrices).
  sorted: If true the resulting <code>k</code> elements will be sorted by the values in
    descending order.
  name: Optional name for the operation.</p>
<p>Returns:
  values: The <code>k</code> largest elements along each last dimensional slice.
  indices: The indices of <code>values</code> within the last dimension of <code>input</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_uniform_candidate_sampler">
    <p>def <span class="ident">map_uniform_candidate_sampler</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.uniform_candidate_sampler</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.uniform_candidate_sampler</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.uniform_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.uniform_candidate_sampler</code> </strong></p>
<div class="codehilite"><pre><span></span>    def uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)
</pre></div>


<p>Samples a set of classes using a uniform base distribution.</p>
<p>This operation randomly samples a tensor of sampled classes
(<code>sampled_candidates</code>) from the range of integers <code>[0, range_max]</code>.</p>
<p>The elements of <code>sampled_candidates</code> are drawn without replacement
(if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from
the base distribution.</p>
<p>The base distribution for this operation is the uniform distribution
over the range of integers <code>[0, range_max]</code>.</p>
<p>In addition, this operation returns tensors <code>true_expected_count</code>
and <code>sampled_expected_count</code> representing the number of times each
of the target classes (<code>true_classes</code>) and the sampled
classes (<code>sampled_candidates</code>) is expected to occur in an average
tensor of sampled classes.  These values correspond to <code>Q(y|x)</code>
defined in <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">this
document</a>.
If <code>unique=True</code>, then these are post-rejection probabilities and we
compute them approximately.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  unique: A <code>bool</code>. Determines whether all sampled classes in a batch are
    unique.
  range_max: An <code>int</code>. The number of possible classes.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled classes.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_weighted_cross_entropy_with_logits">
    <p>def <span class="ident">map_weighted_cross_entropy_with_logits</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.weighted_cross_entropy_with_logits</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.weighted_cross_entropy_with_logits</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.weighted_cross_entropy_with_logits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.weighted_cross_entropy_with_logits</code> </strong></p>
<div class="codehilite"><pre><span></span>    def weighted_cross_entropy_with_logits(logits, targets, pos_weight, name=None)
</pre></div>


<p>Computes a weighted cross entropy.</p>
<p>This is like <code>sigmoid_cross_entropy_with_logits()</code> except that <code>pos_weight</code>,
allows one to trade off recall and precision by up- or down-weighting the
cost of a positive error relative to a negative error.</p>
<p>The usual cross-entropy cost is defined as:</p>
<p>targets * -log(sigmoid(logits)) + (1 - targets) * -log(1 - sigmoid(logits))</p>
<p>The argument <code>pos_weight</code> is used as a multiplier for the positive targets:</p>
<p>targets * -log(sigmoid(logits)) * pos_weight +
      (1 - targets) * -log(1 - sigmoid(logits))</p>
<p>For brevity, let <code>x = logits</code>, <code>z = targets</code>, <code>q = pos_weight</code>.
The loss is:</p>
<div class="codehilite"><pre><span></span>  qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= qz * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= qz * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= qz * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + (qz +  1 - z) * log(1 + exp(-x))
= (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(-x))
</pre></div>


<p>Setting <code>l = (1 + (q - 1) * z)</code>, to ensure stability and avoid overflow,
the implementation uses</p>
<div class="codehilite"><pre><span></span>(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))
</pre></div>


<p><code>logits</code> and <code>targets</code> must have the same type and shape.</p>
<p>Args:
  logits: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.
  targets: A <code>Tensor</code> of the same type and shape as <code>logits</code>.
  pos_weight: A coefficient to use on the positive examples.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of the same shape as <code>logits</code> with the componentwise
  weightedlogistic losses.</p>
<p>Raises:
  ValueError: If <code>logits</code> and <code>targets</code> do not have the same shape.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_xw_plus_b">
    <p>def <span class="ident">map_xw_plus_b</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.xw_plus_b</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.xw_plus_b</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.xw_plus_b can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.xw_plus_b</code> </strong></p>
<div class="codehilite"><pre><span></span>    def xw_plus_b(x, weights, biases, name=None)
</pre></div>


<p>Computes matmul(x, weights) + biases.</p>
<p>Args:
  x: a 2D tensor.  Dimensions typically: batch, in_units
  weights: a 2D tensor.  Dimensions typically: in_units, out_units
  biases: a 1D tensor.  Dimensions: out_units
  name: A name for the operation (optional).  If not specified
    "xw_plus_b" is used.</p>
<p>Returns:
  A 2-D Tensor computing matmul(x, weights) + biases.
  Dimensions typically: batch, out_units.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_xw_plus_b_v1">
    <p>def <span class="ident">map_xw_plus_b_v1</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.xw_plus_b_v1</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.xw_plus_b_v1</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.xw_plus_b_v1 can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.xw_plus_b_v1</code> </strong></p>
<div class="codehilite"><pre><span></span>    def xw_plus_b_v1(x, weights, biases, name=None)
</pre></div>


<p>Computes matmul(x, weights) + biases.</p>
<p>This is a deprecated version of that will soon be removed.</p>
<p>Args:
  x: a 2D tensor.  Dimensions typically: batch, in_units
  weights: a 2D tensor.  Dimensions typically: in_units, out_units
  biases: a 1D tensor.  Dimensions: out_units
  name: A name for the operation (optional).  If not specified
    "xw_plus_b_v1" is used.</p>
<p>Returns:
  A 2-D Tensor computing matmul(x, weights) + biases.
  Dimensions typically: batch, out_units.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.map_zero_fraction">
    <p>def <span class="ident">map_zero_fraction</span>(</p><p>builder, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It maps <code>tf.nn.zero_fraction</code> over the current tensor. All positional (*args) and keyword arguments (**kwargs) are forwarded to <code>tf.nn.zero_fraction</code>.</p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.zero_fraction can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.zero_fraction</code> </strong></p>
<div class="codehilite"><pre><span></span>    def zero_fraction(value, name=None)
</pre></div>


<p>Returns the fraction of zeros in <code>value</code>.</p>
<p>If <code>value</code> is empty, the result is <code>nan</code>.</p>
<p>This is useful in summaries to measure and report sparsity.  For example,</p>
<div class="codehilite"><pre><span></span>z = tf.Relu(...)
summ = tf.scalar_summary(&#39;sparsity&#39;, tf.nn.zero_fraction(z))
</pre></div>


<p>Args:
  value: A tensor of numeric type.
  name: A name for the operation (optional).</p>
<p>Returns:
  The fraction of zeros in <code>value</code>, with type <code>float32</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.max_pool_layer">
    <p>def <span class="ident">max_pool_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.max_pool</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.max_pool</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.max_pool can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.max_pool</code></strong></p>
<div class="codehilite"><pre><span></span>    def max_pool(value, ksize, strides, padding, data_format=&quot;NHWC&quot;, name=None)
</pre></div>


<p>Performs the max pooling on the input.</p>
<p>Args:
  value: A 4-D <code>Tensor</code> with shape <code>[batch, height, width, channels]</code> and
    type <code>tf.float32</code>.
  ksize: A list of ints that has length &gt;= 4.  The size of the window for
    each dimension of the input tensor.
  strides: A list of ints that has length &gt;= 4.  The stride of the sliding
    window for each dimension of the input tensor.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>. The padding algorithm.
  data_format: A string. 'NHWC' and 'NCHW' are supported.
  name: Optional name for the operation.</p>
<p>Returns:
  A <code>Tensor</code> with type <code>tf.float32</code>.  The max pooled output tensor.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.max_pool_with_argmax_layer">
    <p>def <span class="ident">max_pool_with_argmax_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.max_pool_with_argmax</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.max_pool_with_argmax</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.max_pool_with_argmax can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.max_pool_with_argmax</code></strong></p>
<div class="codehilite"><pre><span></span>    def max_pool_with_argmax(input, ksize, strides, padding, Targmax=None, name=None)
</pre></div>


<p>Performs max pooling on the input and outputs both max values and indices.</p>
<p>The indices in <code>argmax</code> are flattened, so that a maximum value at position
<code>[b, y, x, c]</code> becomes flattened index
<code>((b * height + y) * width + x) * channels + c</code>.</p>
<p>Args:
  input: A <code>Tensor</code> of type <code>float32</code>.
    4-D with shape <code>[batch, height, width, channels]</code>.  Input to pool over.
  ksize: A list of <code>ints</code> that has length <code>&gt;= 4</code>.
    The size of the window for each dimension of the input tensor.
  strides: A list of <code>ints</code> that has length <code>&gt;= 4</code>.
    The stride of the sliding window for each dimension of the
    input tensor.
  padding: A <code>string</code> from: <code>"SAME", "VALID"</code>.
    The type of padding algorithm to use.
  Targmax: An optional <code>tf.DType</code> from: <code>tf.int32, tf.int64</code>. Defaults to <code>tf.int64</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A tuple of <code>Tensor</code> objects (output, argmax).
  output: A <code>Tensor</code> of type <code>float32</code>. The max pooled output tensor.
  argmax: A <code>Tensor</code> of type <code>Targmax</code>. 4-D.  The flattened indices of the max values chosen for each output.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.moments_layer">
    <p>def <span class="ident">moments_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.moments</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.moments</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.moments can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.moments</code></strong></p>
<div class="codehilite"><pre><span></span>    def moments(x, axes, name=None, keep_dims=False)
</pre></div>


<p>Calculate the mean and variance of <code>x</code>.</p>
<p>The mean and variance are calculated by aggregating the contents of <code>x</code>
across <code>axes</code>.  If <code>x</code> is 1-D and <code>axes = [0]</code> this is just the mean
and variance of a vector.</p>
<p>When using these moments for batch normalization (see
<code>tf.nn.batch_normalization</code>):
  * for so-called "global normalization", used with convolutional filters with
    shape <code>[batch, height, width, depth]</code>, pass <code>axes=[0, 1, 2]</code>.
  * for simple batch normalization pass <code>axes=[0]</code> (batch only).</p>
<p>Args:
  x: A <code>Tensor</code>.
  axes: array of ints.  Axes along which to compute mean and
    variance.
  keep_dims: produce moments with the same dimensionality as the input.
  name: Name used to scope the operations that compute the moments.</p>
<p>Returns:
  Two <code>Tensor</code> objects: <code>mean</code> and <code>variance</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.nce_loss_layer">
    <p>def <span class="ident">nce_loss_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.nce_loss</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.nce_loss</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.nce_loss can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.nce_loss</code></strong></p>
<div class="codehilite"><pre><span></span>    def nce_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, partition_strategy=&quot;mod&quot;, name=&quot;nce_loss&quot;)
</pre></div>


<p>Computes and returns the noise-contrastive estimation training loss.</p>
<p>See [Noise-contrastive estimation: A new estimation principle for
unnormalized statistical models]
(http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf).
Also see our [Candidate Sampling Algorithms Reference]
(../../extras/candidate_sampling.pdf)</p>
<p>Note: In the case where <code>num_true</code> &gt; 1, we assign to each target class
the target probability 1 / <code>num_true</code> so that the target probabilities
sum to 1 per-example.</p>
<p>Note: It would be useful to allow a variable number of target classes per
example.  We hope to provide this functionality in a future release.
For now, if you have a variable number of target classes, you can pad them
out to a constant number by either repeating them or by padding
with an otherwise unused class.</p>
<p>Args:
  weights: A <code>Tensor</code> of shape <code>[num_classes, dim]</code>, or a list of <code>Tensor</code>
      objects whose concatenation along dimension 0 has shape
      [num_classes, dim].  The (possibly-partitioned) class embeddings.
  biases: A <code>Tensor</code> of shape <code>[num_classes]</code>.  The class biases.
  inputs: A <code>Tensor</code> of shape <code>[batch_size, dim]</code>.  The forward
      activations of the input network.
  labels: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
      num_true]</code>. The target classes.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  num_classes: An <code>int</code>. The number of possible classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  sampled_values: a tuple of (<code>sampled_candidates</code>, <code>true_expected_count</code>,
      <code>sampled_expected_count</code>) returned by a <code>*_candidate_sampler</code> function.
      (if None, we default to <code>log_uniform_candidate_sampler</code>)
  remove_accidental_hits:  A <code>bool</code>.  Whether to remove "accidental hits"
      where a sampled class equals one of the target classes.  If set to
      <code>True</code>, this is a "Sampled Logistic" loss instead of NCE, and we are
      learning to generate log-odds instead of log probabilities.  See
      our [Candidate Sampling Algorithms Reference]
      (../../extras/candidate_sampling.pdf).
      Default is False.
  partition_strategy: A string specifying the partitioning strategy, relevant
      if <code>len(weights) &gt; 1</code>. Currently <code>"div"</code> and <code>"mod"</code> are supported.
      Default is <code>"mod"</code>. See <code>tf.nn.embedding_lookup</code> for more details.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>batch_size</code> 1-D tensor of per-example NCE losses.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.normalize_moments_layer">
    <p>def <span class="ident">normalize_moments_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.normalize_moments</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.normalize_moments</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.normalize_moments can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.normalize_moments</code></strong></p>
<div class="codehilite"><pre><span></span>    def normalize_moments(counts, mean_ss, variance_ss, shift, name=None)
</pre></div>


<p>Calculate the mean and variance of based on the sufficient statistics.</p>
<p>Args:
  counts: A <code>Tensor</code> containing a the total count of the data (one value).
  mean_ss: A <code>Tensor</code> containing the mean sufficient statistics: the (possibly
    shifted) sum of the elements to average over.
  variance_ss: A <code>Tensor</code> containing the variance sufficient statistics: the
    (possibly shifted) squared sum of the data to compute the variance over.
  shift: A <code>Tensor</code> containing the value by which the data is shifted for
    numerical stability, or <code>None</code> if no shift was performed.
  name: Name used to scope the operations that compute the moments.</p>
<p>Returns:
  Two <code>Tensor</code> objects: <code>mean</code> and <code>variance</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.relu6_layer">
    <p>def <span class="ident">relu6_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.relu6</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.relu6</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.relu6 can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.relu6</code></strong></p>
<div class="codehilite"><pre><span></span>    def relu6(features, name=None)
</pre></div>


<p>Computes Rectified Linear 6: <code>min(max(features, 0), 6)</code>.</p>
<p>Args:
  features: A <code>Tensor</code> with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>,
    <code>int16</code>, or <code>int8</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> with the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.relu_layer">
    <p>def <span class="ident">relu_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.relu</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.relu</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.relu can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.relu</code></strong></p>
<div class="codehilite"><pre><span></span>    def relu(features, name=None)
</pre></div>


<p>Computes rectified linear: <code>max(features, 0)</code>.</p>
<p>Args:
  features: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.relu_layer_layer">
    <p>def <span class="ident">relu_layer_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.relu_layer</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.relu_layer</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.relu_layer can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.relu_layer</code></strong></p>
<div class="codehilite"><pre><span></span>    def relu_layer(x, weights, biases, name=None)
</pre></div>


<p>Computes Relu(x * weight + biases).</p>
<p>Args:
  x: a 2D tensor.  Dimensions typically: batch, in_units
  weights: a 2D tensor.  Dimensions typically: in_units, out_units
  biases: a 1D tensor.  Dimensions: out_units
  name: A name for the operation (optional).  If not specified
    "nn_relu_layer" is used.</p>
<p>Returns:
  A 2-D Tensor computing relu(matmul(x, weights) + biases).
  Dimensions typically: batch, out_units.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.rnn_layer">
    <p>def <span class="ident">rnn_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.rnn</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.rnn</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.rnn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.rnn</code></strong></p>
<div class="codehilite"><pre><span></span>    def rnn(cell, inputs, initial_state=None, dtype=None, sequence_length=None, scope=None)
</pre></div>


<p>Creates a recurrent neural network specified by RNNCell "cell".</p>
<p>The simplest form of RNN network generated is:
  state = cell.zero_state(...)
  outputs = []
  for input_ in inputs:
    output, state = cell(input_, state)
    outputs.append(output)
  return (outputs, state)</p>
<p>However, a few other options are available:</p>
<p>An initial state can be provided.
If the sequence_length vector is provided, dynamic calculation is performed.
This method of calculation does not compute the RNN steps past the maximum
sequence length of the minibatch (thus saving computational time),
and properly propagates the state at an example's sequence length
to the final state output.</p>
<p>The dynamic calculation performed is, at time t for batch row b,
  (output, state)(b, t) =
    (t &gt;= sequence_length(b))
      ? (zeros(cell.output_size), states(b, sequence_length(b) - 1))
      : cell(input(b, t), state(b, t - 1))</p>
<p>Args:
  cell: An instance of RNNCell.
  inputs: A length T list of inputs, each a tensor of shape
    [batch_size, cell.input_size].
  initial_state: (optional) An initial state for the RNN.  This must be
    a tensor of appropriate type and shape [batch_size x cell.state_size].
  dtype: (optional) The data type for the initial state.  Required if
    initial_state is not provided.
  sequence_length: Specifies the length of each sequence in inputs.
    An int32 or int64 vector (tensor) size [batch_size].  Values in [0, T).
  scope: VariableScope for the created subgraph; defaults to "RNN".</p>
<p>Returns:
  A pair (outputs, state) where:
    outputs is a length T list of outputs (one for each input)
    state is the final state</p>
<p>Raises:
  TypeError: If "cell" is not an instance of RNNCell.
  ValueError: If inputs is None or an empty list, or if the input depth
    cannot be inferred from inputs via shape inference.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.sampled_softmax_loss_layer">
    <p>def <span class="ident">sampled_softmax_loss_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.sampled_softmax_loss</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.sampled_softmax_loss</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sampled_softmax_loss can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sampled_softmax_loss</code></strong></p>
<div class="codehilite"><pre><span></span>    def sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy=&quot;mod&quot;, name=&quot;sampled_softmax_loss&quot;)
</pre></div>


<p>Computes and returns the sampled softmax training loss.</p>
<p>This is a faster way to train a softmax classifier over a huge number of
classes.</p>
<p>This operation is for training only.  It is generally an underestimate of
the full softmax loss.</p>
<p>At inference time, you can compute full softmax probabilities with the
expression <code>tf.nn.softmax(tf.matmul(inputs, weights) + biases)</code>.</p>
<p>See our [Candidate Sampling Algorithms Reference]
(../../extras/candidate_sampling.pdf)</p>
<p>Also see Section 3 of <a href="http://arxiv.org/abs/1412.2007">Jean et al., 2014</a>
(<a href="http://arxiv.org/pdf/1412.2007.pdf">pdf</a>) for the math.</p>
<p>Args:
  weights: A <code>Tensor</code> of shape <code>[num_classes, dim]</code>, or a list of <code>Tensor</code>
      objects whose concatenation along dimension 0 has shape
      [num_classes, dim].  The (possibly-sharded) class embeddings.
  biases: A <code>Tensor</code> of shape <code>[num_classes]</code>.  The class biases.
  inputs: A <code>Tensor</code> of shape <code>[batch_size, dim]</code>.  The forward
      activations of the input network.
  labels: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
      num_true]</code>. The target classes.  Note that this format differs from
      the <code>labels</code> argument of <code>nn.softmax_cross_entropy_with_logits</code>.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  num_classes: An <code>int</code>. The number of possible classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  sampled_values: a tuple of (<code>sampled_candidates</code>, <code>true_expected_count</code>,
      <code>sampled_expected_count</code>) returned by a <code>*_candidate_sampler</code> function.
      (if None, we default to <code>log_uniform_candidate_sampler</code>)
  remove_accidental_hits:  A <code>bool</code>.  whether to remove "accidental hits"
      where a sampled class equals one of the target classes.  Default is
      True.
  partition_strategy: A string specifying the partitioning strategy, relevant
      if <code>len(weights) &gt; 1</code>. Currently <code>"div"</code> and <code>"mod"</code> are supported.
      Default is <code>"mod"</code>. See <code>tf.nn.embedding_lookup</code> for more details.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>batch_size</code> 1-D tensor of per-example sampled softmax losses.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.separable_conv2d_layer">
    <p>def <span class="ident">separable_conv2d_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.separable_conv2d</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.separable_conv2d</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.separable_conv2d can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.separable_conv2d</code></strong></p>
<div class="codehilite"><pre><span></span>    def separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=None)
</pre></div>


<p>2-D convolution with separable filters.</p>
<p>Performs a depthwise convolution that acts separately on channels followed by
a pointwise convolution that mixes channels.  Note that this is separability
between dimensions <code>[1, 2]</code> and <code>3</code>, not spatial separability between
dimensions <code>1</code> and <code>2</code>.</p>
<p>In detail,</p>
<div class="codehilite"><pre><span></span>output[b, i, j, k] = sum_{di, dj, q, r]
    input[b, strides[1] * i + di, strides[2] * j + dj, q] *
    depthwise_filter[di, dj, q, r] *
    pointwise_filter[0, 0, q * channel_multiplier + r, k]
</pre></div>


<p><code>strides</code> controls the strides for the depthwise convolution only, since
the pointwise convolution has implicit strides of <code>[1, 1, 1, 1]</code>.  Must have
<code>strides[0] = strides[3] = 1</code>.  For the most common case of the same
horizontal and vertical strides, <code>strides = [1, stride, stride, 1]</code>.</p>
<p>Args:
  input: 4-D <code>Tensor</code> with shape <code>[batch, in_height, in_width, in_channels]</code>.
  depthwise_filter: 4-D <code>Tensor</code> with shape
    <code>[filter_height, filter_width, in_channels, channel_multiplier]</code>.
    Contains <code>in_channels</code> convolutional filters of depth 1.
  pointwise_filter: 4-D <code>Tensor</code> with shape
    <code>[1, 1, channel_multiplier * in_channels, out_channels]</code>.  Pointwise
    filter to mix channels after <code>depthwise_filter</code> has convolved spatially.
  strides: 1-D of size 4.  The strides for the depthwise convolution for
    each dimension of <code>input</code>.
  padding: A string, either <code>'VALID'</code> or <code>'SAME'</code>.  The padding algorithm.
  name: A name for this operation (optional).</p>
<p>Returns:
  A 4-D <code>Tensor</code> of shape <code>[batch, out_height, out_width, out_channels]</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.sigmoid_cross_entropy_with_logits_layer">
    <p>def <span class="ident">sigmoid_cross_entropy_with_logits_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.sigmoid_cross_entropy_with_logits</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.sigmoid_cross_entropy_with_logits</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sigmoid_cross_entropy_with_logits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sigmoid_cross_entropy_with_logits</code></strong></p>
<div class="codehilite"><pre><span></span>    def sigmoid_cross_entropy_with_logits(logits, targets, name=None)
</pre></div>


<p>Computes sigmoid cross entropy given <code>logits</code>.</p>
<p>Measures the probability error in discrete classification tasks in which each
class is independent and not mutually exclusive.  For instance, one could
perform multilabel classification where a picture can contain both an elephant
and a dog at the same time.</p>
<p>For brevity, let <code>x = logits</code>, <code>z = targets</code>.  The logistic loss is</p>
<div class="codehilite"><pre><span></span>  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + log(1 + exp(-x))
= x - x * z + log(1 + exp(-x))
</pre></div>


<p>To ensure stability and avoid overflow, the implementation uses</p>
<div class="codehilite"><pre><span></span>max(x, 0) - x * z + log(1 + exp(-abs(x)))
</pre></div>


<p><code>logits</code> and <code>targets</code> must have the same type and shape.</p>
<p>Args:
  logits: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.
  targets: A <code>Tensor</code> of the same type and shape as <code>logits</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of the same shape as <code>logits</code> with the componentwise
  logistic losses.</p>
<p>Raises:
  ValueError: If <code>logits</code> and <code>targets</code> do not have the same shape.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.sigmoid_layer">
    <p>def <span class="ident">sigmoid_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.sigmoid</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.sigmoid</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sigmoid can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sigmoid</code></strong></p>
<div class="codehilite"><pre><span></span>    def sigmoid(x, name=None)
</pre></div>


<p>Computes sigmoid of <code>x</code> element-wise.</p>
<p>Specifically, <code>y = 1 / (1 + exp(-x))</code>.</p>
<p>Args:
  x: A Tensor with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>complex64</code>, <code>int64</code>,
    or <code>qint32</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A Tensor with the same type as <code>x</code> if <code>x.dtype != qint32</code>
    otherwise the return type is <code>quint8</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.softmax_cross_entropy_with_logits_layer">
    <p>def <span class="ident">softmax_cross_entropy_with_logits_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.softmax_cross_entropy_with_logits</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.softmax_cross_entropy_with_logits</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.softmax_cross_entropy_with_logits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.softmax_cross_entropy_with_logits</code></strong></p>
<div class="codehilite"><pre><span></span>    def softmax_cross_entropy_with_logits(logits, labels, name=None)
</pre></div>


<p>Computes softmax cross entropy between <code>logits</code> and <code>labels</code>.</p>
<p>Measures the probability error in discrete classification tasks in which the
classes are mutually exclusive (each entry is in exactly one class).  For
example, each CIFAR-10 image is labeled with one and only one label: an image
can be a dog or a truck, but not both.</p>
<p><strong>NOTE:</strong>  While the classes are mutually exclusive, their probabilities
need not be. If using exclusive <code>labels</code> (wherein one and only one class is
true at a time), see <code>sparse_softmax_cross_entropy_with_logits</code>.</p>
<p><strong>WARNING:</strong> This op expects unscaled logits, since it performs a <code>softmax</code>
on <code>logits</code> internally for efficiency.  Do not call this op with the
output of <code>softmax</code>, as it will produce incorrect results.</p>
<p><code>logits</code> and <code>labels</code> must have the same shape <code>[batch_size, num_classes]</code>
and the same dtype (either <code>float32</code> or <code>float64</code>).</p>
<p>Args:
  logits: Unscaled log probabilities.
  labels: Each row <code>labels[i]</code> must be a valid probability distribution or
      all zeros. If all zeros, the corresponding loss will be <code>0</code>, regardless
      of the contents of <code>logits[i]</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A 1-D <code>Tensor</code> of length <code>batch_size</code> of the same type as <code>logits</code> with the
  softmax cross entropy loss.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.softmax_layer">
    <p>def <span class="ident">softmax_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.softmax</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.softmax</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.softmax can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.softmax</code></strong></p>
<div class="codehilite"><pre><span></span>    def softmax(logits, name=None)
</pre></div>


<p>Computes softmax activations.</p>
<p>For each batch <code>i</code> and class <code>j</code> we have</p>
<div class="codehilite"><pre><span></span>softmax[i, j] = exp(logits[i, j]) / sum(exp(logits[i]))
</pre></div>


<p>Args:
  logits: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>.
    2-D with shape <code>[batch_size, num_classes]</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>logits</code>. Same shape as <code>logits</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.softplus_layer">
    <p>def <span class="ident">softplus_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.softplus</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.softplus</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.softplus can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.softplus</code></strong></p>
<div class="codehilite"><pre><span></span>    def softplus(features, name=None)
</pre></div>


<p>Computes softplus: <code>log(exp(features) + 1)</code>.</p>
<p>Args:
  features: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.softsign_layer">
    <p>def <span class="ident">softsign_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.softsign</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.softsign</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.softsign can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.softsign</code></strong></p>
<div class="codehilite"><pre><span></span>    def softsign(features, name=None)
</pre></div>


<p>Computes softsign: <code>features / (abs(features) + 1)</code>.</p>
<p>Args:
  features: A <code>Tensor</code>. Must be one of the following types: <code>float32</code>, <code>float64</code>, <code>int32</code>, <code>int64</code>, <code>uint8</code>, <code>int16</code>, <code>int8</code>, <code>uint16</code>, <code>half</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code>. Has the same type as <code>features</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.sparse_softmax_cross_entropy_with_logits_layer">
    <p>def <span class="ident">sparse_softmax_cross_entropy_with_logits_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sparse_softmax_cross_entropy_with_logits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></strong></p>
<div class="codehilite"><pre><span></span>    def sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)
</pre></div>


<p>Computes sparse softmax cross entropy between <code>logits</code> and <code>labels</code>.</p>
<p>Measures the probability error in discrete classification tasks in which the
classes are mutually exclusive (each entry is in exactly one class).  For
example, each CIFAR-10 image is labeled with one and only one label: an image
can be a dog or a truck, but not both.</p>
<p><strong>NOTE:</strong>  For this operation, the probability of a given label is considered
exclusive.  That is, soft classes are not allowed, and the <code>labels</code> vector
must provide a single specific index for the true class for each row of
<code>logits</code> (each minibatch entry).  For soft softmax classification with
a probability distribution for each entry, see
<code>softmax_cross_entropy_with_logits</code>.</p>
<p><strong>WARNING:</strong> This op expects unscaled logits, since it performs a <code>softmax</code>
on <code>logits</code> internally for efficiency.  Do not call this op with the
output of <code>softmax</code>, as it will produce incorrect results.</p>
<p><code>logits</code> and must have the shape <code>[batch_size, num_classes]</code>
and the dtype (either <code>float32</code> or <code>float64</code>).</p>
<p><code>labels</code> must have the shape <code>[batch_size]</code> and the dtype <code>int64</code>.</p>
<p>Args:
  logits: Unscaled log probabilities.
  labels: Each entry <code>labels[i]</code> must be an index in <code>[0, num_classes)</code> or
      <code>-1</code>. If <code>-1</code>, the corresponding loss will be <code>0</code>, regardless
      of the contents of <code>logits[i]</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A 1-D <code>Tensor</code> of length <code>batch_size</code> of the same type as <code>logits</code> with the
  softmax cross entropy loss.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.state_saving_rnn_layer">
    <p>def <span class="ident">state_saving_rnn_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.state_saving_rnn</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.state_saving_rnn</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.state_saving_rnn can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.state_saving_rnn</code></strong></p>
<div class="codehilite"><pre><span></span>    def state_saving_rnn(cell, inputs, state_saver, state_name, sequence_length=None, scope=None)
</pre></div>


<p>RNN that accepts a state saver for time-truncated RNN calculation.</p>
<p>Args:
  cell: An instance of RNNCell.
  inputs: A length T list of inputs, each a tensor of shape
    [batch_size, cell.input_size].
  state_saver: A state saver object with methods <code>state</code> and <code>save_state</code>.
  state_name: The name to use with the state_saver.
  sequence_length: (optional) An int32/int64 vector size [batch_size].
    See the documentation for rnn() for more details about sequence_length.
  scope: VariableScope for the created subgraph; defaults to "RNN".</p>
<p>Returns:
  A pair (outputs, state) where:
    outputs is a length T list of outputs (one for each input)
    states is the final state</p>
<p>Raises:
  TypeError: If "cell" is not an instance of RNNCell.
  ValueError: If inputs is None or an empty list.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.sufficient_statistics_layer">
    <p>def <span class="ident">sufficient_statistics_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.sufficient_statistics</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.sufficient_statistics</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.sufficient_statistics can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.sufficient_statistics</code></strong></p>
<div class="codehilite"><pre><span></span>    def sufficient_statistics(x, axes, shift=True, keep_dims=False, name=None)
</pre></div>


<p>Calculate the sufficient statistics for the mean and variance of <code>x</code>.</p>
<p>These sufficient statistics are computed using the one pass algorithm on
an input that's optionally shifted using the value of the 1st element in <code>x</code>.
See:
https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data</p>
<p>Args:
  x: A <code>Tensor</code>.
  axes: Array of ints. Axes along which to compute mean and variance.
  shift: If true, shift the data to provide more numerically stable results.
  keep_dims: produce statistics with the same dimensionality as the input.
  name: Name used to scope the operations that compute the sufficient stats.</p>
<p>Returns:
  Four <code>Tensor</code> objects of the same type as <code>x</code>:
  * the count (number of elements to average over).
  * the (possibly shifted) sum of the elements in the array.
  * the (possibly shifted) sum of squares of the elements in the array.
  * the shift by which the mean must be corrected or None if <code>shift</code> is False.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.tanh_layer">
    <p>def <span class="ident">tanh_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.tanh</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.tanh</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.tanh can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.tanh</code></strong></p>
<div class="codehilite"><pre><span></span>    def tanh(x, name=None)
</pre></div>


<p>Computes hyperbolic tangent of <code>x</code> element-wise.</p>
<p>Args:
  x: A Tensor with type <code>float</code>, <code>double</code>, <code>int32</code>, <code>complex64</code>, <code>int64</code>,
    or <code>qint32</code>.
  name: A name for the operation (optional).</p>
<p>Returns:
  A Tensor with the same type as <code>x</code> if <code>x.dtype != qint32</code> otherwise
    the return type is <code>quint8</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.top_k_layer">
    <p>def <span class="ident">top_k_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.top_k</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.top_k</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.top_k can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.top_k</code></strong></p>
<div class="codehilite"><pre><span></span>    def top_k(input, k=1, sorted=True, name=None)
</pre></div>


<p>Finds values and indices of the <code>k</code> largest entries for the last dimension.</p>
<p>If the input is a vector (rank-1), finds the <code>k</code> largest entries in the vector
and outputs their values and indices as vectors.  Thus <code>values[j]</code> is the
<code>j</code>-th largest entry in <code>input</code>, and its index is <code>indices[j]</code>.</p>
<p>For matrices (resp. higher rank input), computes the top <code>k</code> entries in each
row (resp. vector along the last dimension).  Thus,</p>
<div class="codehilite"><pre><span></span><span class="s s-Atom">values</span><span class="p">.</span><span class="s s-Atom">shape</span> <span class="o">=</span> <span class="s s-Atom">indices</span><span class="p">.</span><span class="s s-Atom">shape</span> <span class="o">=</span> <span class="s s-Atom">input</span><span class="p">.</span><span class="s s-Atom">shape</span><span class="p">[:-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="s s-Atom">k</span><span class="p">]</span>
</pre></div>


<p>If two elements are equal, the lower-index element appears first.</p>
<p>Args:
  input: 1-D or higher <code>Tensor</code> with last dimension at least <code>k</code>.
  k: 0-D <code>int32</code> <code>Tensor</code>.  Number of top elements to look for along the last
    dimension (along each row for matrices).
  sorted: If true the resulting <code>k</code> elements will be sorted by the values in
    descending order.
  name: Optional name for the operation.</p>
<p>Returns:
  values: The <code>k</code> largest elements along each last dimensional slice.
  indices: The indices of <code>values</code> within the last dimension of <code>input</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.uniform_candidate_sampler_layer">
    <p>def <span class="ident">uniform_candidate_sampler_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.uniform_candidate_sampler</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.uniform_candidate_sampler</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.uniform_candidate_sampler can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.uniform_candidate_sampler</code></strong></p>
<div class="codehilite"><pre><span></span>    def uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None)
</pre></div>


<p>Samples a set of classes using a uniform base distribution.</p>
<p>This operation randomly samples a tensor of sampled classes
(<code>sampled_candidates</code>) from the range of integers <code>[0, range_max]</code>.</p>
<p>The elements of <code>sampled_candidates</code> are drawn without replacement
(if <code>unique=True</code>) or with replacement (if <code>unique=False</code>) from
the base distribution.</p>
<p>The base distribution for this operation is the uniform distribution
over the range of integers <code>[0, range_max]</code>.</p>
<p>In addition, this operation returns tensors <code>true_expected_count</code>
and <code>sampled_expected_count</code> representing the number of times each
of the target classes (<code>true_classes</code>) and the sampled
classes (<code>sampled_candidates</code>) is expected to occur in an average
tensor of sampled classes.  These values correspond to <code>Q(y|x)</code>
defined in <a href="http://www.tensorflow.org/extras/candidate_sampling.pdf">this
document</a>.
If <code>unique=True</code>, then these are post-rejection probabilities and we
compute them approximately.</p>
<p>Args:
  true_classes: A <code>Tensor</code> of type <code>int64</code> and shape <code>[batch_size,
    num_true]</code>. The target classes.
  num_true: An <code>int</code>.  The number of target classes per training example.
  num_sampled: An <code>int</code>.  The number of classes to randomly sample per batch.
  unique: A <code>bool</code>. Determines whether all sampled classes in a batch are
    unique.
  range_max: An <code>int</code>. The number of possible classes.
  seed: An <code>int</code>. An operation-specific seed. Default is 0.
  name: A name for the operation (optional).</p>
<p>Returns:
  sampled_candidates: A tensor of type <code>int64</code> and shape <code>[num_sampled]</code>.
    The sampled classes.
  true_expected_count: A tensor of type <code>float</code>.  Same shape as
    <code>true_classes</code>. The expected counts under the sampling distribution
    of each of <code>true_classes</code>.
  sampled_expected_count: A tensor of type <code>float</code>. Same shape as
    <code>sampled_candidates</code>. The expected counts under the sampling distribution
    of each of <code>sampled_candidates</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.use_extras">
    <p>def <span class="ident">use_extras</span>(</p><p>)</p>
    </div>
    

    
  
    <div class="desc"><p>Moneky-patches all functions in this modules as methods on the Builder class.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-tensorbuilder.builder_nn.use_extras', this);">Show source &equiv;</a></p>
  <div id="source-tensorbuilder.builder_nn.use_extras" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">use_extras</span><span class="p">():</span>
	<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">	Moneky-patches all functions in this modules as methods on the Builder class.</span>
<span class="sd">	&quot;&quot;&quot;</span>
	<span class="k">for</span> <span class="n">patch</span> <span class="ow">in</span> <span class="n">_patches</span><span class="p">:</span>
		<span class="k">exec</span><span class="p">(</span><span class="n">patch</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.weighted_cross_entropy_with_logits_layer">
    <p>def <span class="ident">weighted_cross_entropy_with_logits_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.weighted_cross_entropy_with_logits</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.weighted_cross_entropy_with_logits</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.weighted_cross_entropy_with_logits can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.weighted_cross_entropy_with_logits</code></strong></p>
<div class="codehilite"><pre><span></span>    def weighted_cross_entropy_with_logits(logits, targets, pos_weight, name=None)
</pre></div>


<p>Computes a weighted cross entropy.</p>
<p>This is like <code>sigmoid_cross_entropy_with_logits()</code> except that <code>pos_weight</code>,
allows one to trade off recall and precision by up- or down-weighting the
cost of a positive error relative to a negative error.</p>
<p>The usual cross-entropy cost is defined as:</p>
<p>targets * -log(sigmoid(logits)) + (1 - targets) * -log(1 - sigmoid(logits))</p>
<p>The argument <code>pos_weight</code> is used as a multiplier for the positive targets:</p>
<p>targets * -log(sigmoid(logits)) * pos_weight +
      (1 - targets) * -log(1 - sigmoid(logits))</p>
<p>For brevity, let <code>x = logits</code>, <code>z = targets</code>, <code>q = pos_weight</code>.
The loss is:</p>
<div class="codehilite"><pre><span></span>  qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= qz * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= qz * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= qz * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + (qz +  1 - z) * log(1 + exp(-x))
= (1 - z) * x + (1 + (q - 1) * z) * log(1 + exp(-x))
</pre></div>


<p>Setting <code>l = (1 + (q - 1) * z)</code>, to ensure stability and avoid overflow,
the implementation uses</p>
<div class="codehilite"><pre><span></span>(1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))
</pre></div>


<p><code>logits</code> and <code>targets</code> must have the same type and shape.</p>
<p>Args:
  logits: A <code>Tensor</code> of type <code>float32</code> or <code>float64</code>.
  targets: A <code>Tensor</code> of the same type and shape as <code>logits</code>.
  pos_weight: A coefficient to use on the positive examples.
  name: A name for the operation (optional).</p>
<p>Returns:
  A <code>Tensor</code> of the same shape as <code>logits</code> with the componentwise
  weightedlogistic losses.</p>
<p>Raises:
  ValueError: If <code>logits</code> and <code>targets</code> do not have the same shape.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.xw_plus_b_layer">
    <p>def <span class="ident">xw_plus_b_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.xw_plus_b</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.xw_plus_b</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.xw_plus_b can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.xw_plus_b</code></strong></p>
<div class="codehilite"><pre><span></span>    def xw_plus_b(x, weights, biases, name=None)
</pre></div>


<p>Computes matmul(x, weights) + biases.</p>
<p>Args:
  x: a 2D tensor.  Dimensions typically: batch, in_units
  weights: a 2D tensor.  Dimensions typically: in_units, out_units
  biases: a 1D tensor.  Dimensions: out_units
  name: A name for the operation (optional).  If not specified
    "xw_plus_b" is used.</p>
<p>Returns:
  A 2-D Tensor computing matmul(x, weights) + biases.
  Dimensions typically: batch, out_units.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.xw_plus_b_v1_layer">
    <p>def <span class="ident">xw_plus_b_v1_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.xw_plus_b_v1</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.xw_plus_b_v1</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.xw_plus_b_v1 can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.xw_plus_b_v1</code></strong></p>
<div class="codehilite"><pre><span></span>    def xw_plus_b_v1(x, weights, biases, name=None)
</pre></div>


<p>Computes matmul(x, weights) + biases.</p>
<p>This is a deprecated version of that will soon be removed.</p>
<p>Args:
  x: a 2D tensor.  Dimensions typically: batch, in_units
  weights: a 2D tensor.  Dimensions typically: in_units, out_units
  biases: a 1D tensor.  Dimensions: out_units
  name: A name for the operation (optional).  If not specified
    "xw_plus_b_v1" is used.</p>
<p>Returns:
  A 2-D Tensor computing matmul(x, weights) + biases.
  Dimensions typically: batch, out_units.</p></div>
  <div class="source_cont">
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="tensorbuilder.builder_nn.zero_fraction_layer">
    <p>def <span class="ident">zero_fraction_layer</span>(</p><p>builder, size, *args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>THIS METHOD IS AUTOMATICALLY GENERATED</p>
<p><strong>@_immutable</strong></p>
<p>It connects the current tensor to a layer whos output function is <code>tf.nn.zero_fraction</code>. The keyword parameters <code>weights_name</code>, <code>bias</code> and <code>bias_name</code> are set to defaults if they are not present in **kwargs. Any additional positional (*args) and keyword arguments (**kwargs) will be forwarded to <code>tf.nn.zero_fraction</code>. </p>
<p><strong>Note:</strong> </p>
<p>Its expected that tf.nn.zero_fraction can receive <code>builder.tensor</code> as a first parameter.</p>
<p><strong> TensorFlow documentation for <code>tf.nn.zero_fraction</code></strong></p>
<div class="codehilite"><pre><span></span>    def zero_fraction(value, name=None)
</pre></div>


<p>Returns the fraction of zeros in <code>value</code>.</p>
<p>If <code>value</code> is empty, the result is <code>nan</code>.</p>
<p>This is useful in summaries to measure and report sparsity.  For example,</p>
<div class="codehilite"><pre><span></span>z = tf.Relu(...)
summ = tf.scalar_summary(&#39;sparsity&#39;, tf.nn.zero_fraction(z))
</pre></div>


<p>Args:
  value: A tensor of numeric type.
  name: A name for the operation (optional).</p>
<p>Returns:
  The fraction of zeros in <code>value</code>, with type <code>float32</code>.</p></div>
  <div class="source_cont">
</div>

  </div>
  

    <h2 class="section-title" id="header-classes">Classes</h2>
      
      <div class="item">
      <p id="tensorbuilder.builder_nn.DefaultArgSpec" class="name">class <span class="ident">DefaultArgSpec</span></p>
      
  
    <div class="desc"><p>DefaultArgSpec(has_default, default_value)</p></div>
  <div class="source_cont">
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#tensorbuilder.builder_nn.DefaultArgSpec">DefaultArgSpec</a></li>
          <li>__builtin__.tuple</li>
          <li>__builtin__.object</li>
          </ul>
          <h3>Instance variables</h3>
            <div class="item">
            <p id="tensorbuilder.builder_nn.DefaultArgSpec.default_value" class="name">var <span class="ident">default_value</span></p>
            

            
  
    <div class="desc"><p>Alias for field number 1</p></div>
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="tensorbuilder.builder_nn.DefaultArgSpec.has_default" class="name">var <span class="ident">has_default</span></p>
            

            
  
    <div class="desc"><p>Alias for field number 0</p></div>
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>

  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>pdoc is in the public domain with the
      <a href="http://unlicense.org">UNLICENSE</a></p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
